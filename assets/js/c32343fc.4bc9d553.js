"use strict";(globalThis.webpackChunkphysical_ai_humanoid_robotics_book=globalThis.webpackChunkphysical_ai_humanoid_robotics_book||[]).push([[6802],{427:(n,e,r)=>{r.r(e),r.d(e,{assets:()=>l,contentTitle:()=>t,default:()=>p,frontMatter:()=>a,metadata:()=>i,toc:()=>c});const i=JSON.parse('{"id":"chapters/module-4-vla/humanoid-skills","title":"Chapter 4: Humanoid Skills with VLA","description":"Manipulation, locomotion, and multi-task learning for humanoid robots","source":"@site/docs/chapters/module-4-vla/04-humanoid-skills.md","sourceDirName":"chapters/module-4-vla","slug":"/chapters/module-4-vla/humanoid-skills","permalink":"/Physical-AI-Humanoid-Robotics/chapters/module-4-vla/humanoid-skills","draft":false,"unlisted":false,"editUrl":"https://github.com/ayeshadev283-max/Physical-AI-Humanoid-Robotics/tree/main/docs/chapters/module-4-vla/04-humanoid-skills.md","tags":[{"inline":true,"label":"humanoid","permalink":"/Physical-AI-Humanoid-Robotics/tags/humanoid"},{"inline":true,"label":"manipulation","permalink":"/Physical-AI-Humanoid-Robotics/tags/manipulation"},{"inline":true,"label":"locomotion","permalink":"/Physical-AI-Humanoid-Robotics/tags/locomotion"},{"inline":true,"label":"multi-task","permalink":"/Physical-AI-Humanoid-Robotics/tags/multi-task"},{"inline":true,"label":"vla","permalink":"/Physical-AI-Humanoid-Robotics/tags/vla"}],"version":"current","sidebarPosition":4,"frontMatter":{"sidebar_position":4,"title":"Chapter 4: Humanoid Skills with VLA","description":"Manipulation, locomotion, and multi-task learning for humanoid robots","tags":["humanoid","manipulation","locomotion","multi-task","vla"]},"sidebar":"bookSidebar","previous":{"title":"Chapter 3: Policy Integration","permalink":"/Physical-AI-Humanoid-Robotics/chapters/module-4-vla/policy-integration"},"next":{"title":"Module 5: Capstone","permalink":"/Physical-AI-Humanoid-Robotics/module-5-capstone"}}');var s=r(74848),o=r(28453);const a={sidebar_position:4,title:"Chapter 4: Humanoid Skills with VLA",description:"Manipulation, locomotion, and multi-task learning for humanoid robots",tags:["humanoid","manipulation","locomotion","multi-task","vla"]},t="Chapter 4: Humanoid Skills with VLA",l={},c=[{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"4.1 Manipulation Skills",id:"41-manipulation-skills",level:2},{value:"Grasping with VLA",id:"grasping-with-vla",level:3},{value:"Dexterous Manipulation",id:"dexterous-manipulation",level:3},{value:"Tool Use",id:"tool-use",level:3},{value:"4.2 Locomotion Skills",id:"42-locomotion-skills",level:2},{value:"VLA for Navigation",id:"vla-for-navigation",level:3},{value:"Whole-Body Manipulation",id:"whole-body-manipulation",level:3},{value:"Mobile Manipulation",id:"mobile-manipulation",level:3},{value:"4.3 Multi-Task Learning",id:"43-multi-task-learning",level:2},{value:"Task Composition",id:"task-composition",level:3},{value:"Multi-Task VLA Training",id:"multi-task-vla-training",level:3},{value:"Transfer Learning",id:"transfer-learning",level:3},{value:"Continual Learning",id:"continual-learning",level:3},{value:"Exercises",id:"exercises",level:2},{value:"Summary",id:"summary",level:2}];function d(n){const e={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",mermaid:"mermaid",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,o.R)(),...n.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(e.header,{children:(0,s.jsx)(e.h1,{id:"chapter-4-humanoid-skills-with-vla",children:"Chapter 4: Humanoid Skills with VLA"})}),"\n",(0,s.jsx)(e.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,s.jsxs)(e.ol,{children:["\n",(0,s.jsx)(e.li,{children:"Apply VLA models to manipulation tasks (grasping, dexterous manipulation)"}),"\n",(0,s.jsx)(e.li,{children:"Integrate VLA with locomotion policies for mobile manipulation"}),"\n",(0,s.jsx)(e.li,{children:"Implement multi-task learning and skill composition for humanoid robots"}),"\n"]}),"\n",(0,s.jsx)(e.h2,{id:"41-manipulation-skills",children:"4.1 Manipulation Skills"}),"\n",(0,s.jsx)(e.h3,{id:"grasping-with-vla",children:"Grasping with VLA"}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Task"}),': "Grasp the red mug from the table"']}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"VLA Output"}),": 6-DOF gripper pose + gripper state"]}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Pipeline"}),":"]}),"\n",(0,s.jsx)(e.mermaid,{value:"graph LR\r\n    Image[Camera Image<br/>RGB-D] --\x3e VLA[VLA Model<br/>OpenVLA]\r\n    Instruction[\"Instruction<br/>'grasp red mug'\"] --\x3e VLA\r\n\r\n    VLA --\x3e Pose[Gripper Pose<br/>x,y,z,r,p,y]\r\n    VLA --\x3e GripperState[Gripper State<br/>open/close]\r\n\r\n    Pose --\x3e IK[Inverse Kinematics]\r\n    IK --\x3e JointCmd[Joint Commands]\r\n\r\n    JointCmd --\x3e Controller[Arm Controller]\r\n    GripperState --\x3e Controller\r\n\r\n    Controller --\x3e Robot[Humanoid Arm]\r\n\r\n    style VLA fill:#e1f5ff\r\n    style IK fill:#ffe1e1"}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Figure 4.1"}),": VLA-based grasping pipeline showing conversion from end-effector pose to joint commands via inverse kinematics."]}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Implementation"}),":"]}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-python",children:'import numpy as np\r\nfrom openvla import OpenVLA\r\nfrom scipy.spatial.transform import Rotation\r\n\r\nclass VLAGrasper:\r\n    def __init__(self, robot_interface):\r\n        self.vla = OpenVLA.from_pretrained("openvla-7b").to("cuda")\r\n        self.robot = robot_interface\r\n\r\n    def grasp_object(self, image, instruction):\r\n        """\r\n        Execute grasp using VLA\r\n\r\n        Args:\r\n            image: RGB image (224, 224, 3)\r\n            instruction: str, e.g., "grasp the red mug"\r\n\r\n        Returns:\r\n            success: bool\r\n        """\r\n        # 1. Predict grasp pose with VLA\r\n        action = self.vla.predict_action(image, instruction)\r\n        pose = action[:6]  # x, y, z, roll, pitch, yaw\r\n        gripper_cmd = action[6]  # 0-1 (open-close)\r\n\r\n        # 2. Pre-grasp pose (10cm above target)\r\n        pregrasp_pose = pose.copy()\r\n        pregrasp_pose[2] += 0.1  # Lift z by 10cm\r\n\r\n        # 3. Execute motion sequence\r\n        # Step 1: Move to pre-grasp\r\n        self.robot.move_to_pose(pregrasp_pose, speed=0.2)\r\n\r\n        # Step 2: Open gripper\r\n        self.robot.set_gripper(0.0)  # Fully open\r\n\r\n        # Step 3: Move to grasp pose\r\n        self.robot.move_to_pose(pose, speed=0.1)\r\n\r\n        # Step 4: Close gripper\r\n        self.robot.set_gripper(gripper_cmd)\r\n\r\n        # Step 5: Lift object\r\n        lift_pose = pose.copy()\r\n        lift_pose[2] += 0.15\r\n        self.robot.move_to_pose(lift_pose, speed=0.1)\r\n\r\n        # 6. Check grasp success (force sensor)\r\n        force = self.robot.get_gripper_force()\r\n        success = force > 1.0  # Object held\r\n\r\n        return success\n'})}),"\n",(0,s.jsx)(e.h3,{id:"dexterous-manipulation",children:"Dexterous Manipulation"}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Task"}),': "Open the drawer" (requires force control, multi-step reasoning)']}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Challenges"}),":"]}),"\n",(0,s.jsxs)(e.ol,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Contact-rich"}),": Gripper must maintain contact with handle"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Force control"}),": Pull with appropriate force (not too hard/soft)"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Multi-step"}),": (1) Grasp handle, (2) Pull, (3) Release"]}),"\n"]}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"VLA + Impedance Control"}),":"]}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-python",children:'class DexterousManipulator:\r\n    def __init__(self, vla_model, robot):\r\n        self.vla = vla_model\r\n        self.robot = robot\r\n\r\n    def open_drawer(self, image, instruction="open the drawer"):\r\n        """\r\n        Complex manipulation with force feedback\r\n\r\n        Strategy:\r\n        1. VLA predicts grasp pose and pull direction\r\n        2. Impedance controller executes pull with force limits\r\n        """\r\n        # VLA prediction\r\n        action = self.vla.predict_action(image, instruction)\r\n        handle_pose = action[:6]\r\n        pull_direction = action[6:9]  # 3D vector (typically [-1, 0, 0])\r\n\r\n        # Grasp handle\r\n        self.robot.move_to_pose(handle_pose)\r\n        self.robot.set_gripper(1.0)  # Close\r\n\r\n        # Impedance control for pulling\r\n        # Low stiffness in pull direction, high in others\r\n        stiffness = np.array([100, 1000, 1000, 100, 100, 100])  # [x, y, z, rx, ry, rz]\r\n        damping = 2 * np.sqrt(stiffness)  # Critical damping\r\n\r\n        # Pull for 2 seconds or until drawer opens\r\n        start_time = time.time()\r\n        while time.time() - start_time < 2.0:\r\n            # Current pose\r\n            current_pose = self.robot.get_ee_pose()\r\n\r\n            # Desired pose (incremental pull)\r\n            desired_pose = current_pose + 0.01 * pull_direction\r\n\r\n            # Impedance control law\r\n            pose_error = desired_pose - current_pose\r\n            force_cmd = stiffness * pose_error - damping * self.robot.get_ee_velocity()\r\n\r\n            # Execute\r\n            self.robot.apply_ee_force(force_cmd)\r\n\r\n            # Check if drawer opened (depth camera feedback)\r\n            if self.check_drawer_open(image_stream):\r\n                break\r\n\r\n        # Release handle\r\n        self.robot.set_gripper(0.0)\r\n\r\n        return self.check_drawer_open(self.robot.get_camera_image())\n'})}),"\n",(0,s.jsx)(e.h3,{id:"tool-use",children:"Tool Use"}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Task"}),': "Use the hammer to hit the nail"']}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"VLA Capabilities"}),":"]}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"Affordance reasoning: Identifies hammer as tool for hammering"}),"\n",(0,s.jsx)(e.li,{children:"Grasp planning: Grips hammer handle (not head)"}),"\n",(0,s.jsx)(e.li,{children:"Motion planning: Swinging motion to strike nail"}),"\n"]}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-python",children:'def use_tool(image, instruction="use the hammer"):\r\n    # VLA predicts:\r\n    # 1. Tool to grasp (bounding box + grasp pose)\r\n    # 2. Tool-use motion (trajectory waypoints)\r\n\r\n    action = vla.predict_action(image, instruction)\r\n\r\n    # Parse action\r\n    tool_grasp_pose = action[:6]\r\n    motion_waypoints = action[6:].reshape(-1, 6)  # N waypoints\r\n\r\n    # Execute\r\n    robot.move_to_pose(tool_grasp_pose)\r\n    robot.set_gripper(1.0)\r\n\r\n    for waypoint in motion_waypoints:\r\n        robot.move_to_pose(waypoint, speed=0.3)\r\n\r\n    robot.set_gripper(0.0)  # Release tool\n'})}),"\n",(0,s.jsx)(e.h2,{id:"42-locomotion-skills",children:"4.2 Locomotion Skills"}),"\n",(0,s.jsx)(e.h3,{id:"vla-for-navigation",children:"VLA for Navigation"}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Task"}),': "Walk to the kitchen"']}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Challenge"}),": VLA trained on manipulation data may not generalize to locomotion"]}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Solution"}),": Hierarchical control"]}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"High-level"}),' (VLA): Goal selection ("go to kitchen")']}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Low-level"})," (RL policy): Footstep planning and balance"]}),"\n"]}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-python",children:'class HierarchicalNavigator:\r\n    def __init__(self, vla_high_level, rl_low_level):\r\n        self.vla = vla_high_level  # Trained on navigation tasks\r\n        self.rl_policy = rl_low_level  # Trained in Isaac Gym\r\n\r\n    def navigate_to_goal(self, image, instruction="go to the kitchen"):\r\n        """\r\n        VLA: Image \u2192 Waypoints\r\n        RL: Waypoints \u2192 Joint torques\r\n        """\r\n        # VLA predicts waypoint path\r\n        waypoints = self.vla.predict_waypoints(image, instruction)\r\n        # waypoints: [(x1, y1), (x2, y2), ..., (xn, yn)]\r\n\r\n        # RL policy tracks waypoints\r\n        for waypoint in waypoints:\r\n            while not self.reached_waypoint(waypoint):\r\n                # Get robot state\r\n                state = self.get_state()  # Joint angles, IMU, foot contacts\r\n\r\n                # RL policy computes joint torques\r\n                torques = self.rl_policy.predict(state, goal=waypoint)\r\n\r\n                # Execute\r\n                self.robot.set_joint_torques(torques)\r\n                time.sleep(0.01)  # 100 Hz control\n'})}),"\n",(0,s.jsx)(e.h3,{id:"whole-body-manipulation",children:"Whole-Body Manipulation"}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Task"}),': "Pick up the box from the floor while walking"']}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Requirements"}),":"]}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"Locomotion: Maintain balance while walking"}),"\n",(0,s.jsx)(e.li,{children:"Manipulation: Reach down to grasp box"}),"\n",(0,s.jsx)(e.li,{children:"Coordination: Synchronize arm and leg movements"}),"\n"]}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Whole-Body VLA"}),":"]}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-python",children:"class WholeBodyVLA:\r\n    def __init__(self):\r\n        # Shared vision-language encoder\r\n        self.encoder = VisionLanguageEncoder()\r\n\r\n        # Separate heads for arms and legs\r\n        self.arm_head = ActionHead(action_dim=14)  # 7-DOF x2 arms\r\n        self.leg_head = ActionHead(action_dim=12)  # 6-DOF x2 legs\r\n\r\n    def forward(self, image, instruction):\r\n        # Shared encoding\r\n        features = self.encoder(image, instruction)\r\n\r\n        # Predict arm and leg actions\r\n        arm_actions = self.arm_head(features)  # (14,)\r\n        leg_actions = self.leg_head(features)  # (12,)\r\n\r\n        return {\r\n            'arms': arm_actions,\r\n            'legs': leg_actions\r\n        }\r\n\r\n# Usage\r\nwhole_body = WholeBodyVLA()\r\nactions = whole_body(image, \"pick up box while walking forward\")\r\n\r\n# Execute coordinated motion\r\nrobot.set_arm_positions(actions['arms'])\r\nrobot.set_leg_positions(actions['legs'])\n"})}),"\n",(0,s.jsx)(e.h3,{id:"mobile-manipulation",children:"Mobile Manipulation"}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Task"}),': "Navigate to the table and pick up the cup"']}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Two-stage approach"}),":"]}),"\n",(0,s.jsxs)(e.ol,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Stage 1"}),": Navigate to table (locomotion VLA)"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Stage 2"}),": Pick up cup (manipulation VLA)"]}),"\n"]}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-python",children:"def mobile_manipulation(image, instruction=\"go to table and pick up cup\"):\r\n    # Parse instruction into subtasks\r\n    subtasks = parse_instruction(instruction)\r\n    # ['navigate to table', 'pick up cup']\r\n\r\n    for subtask in subtasks:\r\n        if 'navigate' in subtask or 'go to' in subtask:\r\n            # Locomotion\r\n            navigate_to_goal(image, subtask)\r\n        elif 'pick up' in subtask or 'grasp' in subtask:\r\n            # Manipulation\r\n            grasp_object(image, subtask)\r\n        else:\r\n            # General VLA (handles both)\r\n            execute_action(image, subtask)\n"})}),"\n",(0,s.jsx)(e.h2,{id:"43-multi-task-learning",children:"4.3 Multi-Task Learning"}),"\n",(0,s.jsx)(e.h3,{id:"task-composition",children:"Task Composition"}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Primitive Skills"}),":"]}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:(0,s.jsx)(e.code,{children:"grasp(object)"})}),"\n",(0,s.jsx)(e.li,{children:(0,s.jsx)(e.code,{children:"place(location)"})}),"\n",(0,s.jsx)(e.li,{children:(0,s.jsx)(e.code,{children:"navigate(goal)"})}),"\n",(0,s.jsx)(e.li,{children:(0,s.jsx)(e.code,{children:"open(container)"})}),"\n"]}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Composed Tasks"}),":"]}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:['"Pick and place" = ',(0,s.jsx)(e.code,{children:"grasp(cup)"})," \u2192 ",(0,s.jsx)(e.code,{children:"navigate(table)"})," \u2192 ",(0,s.jsx)(e.code,{children:"place(table)"})]}),"\n",(0,s.jsxs)(e.li,{children:['"Fetch and deliver" = ',(0,s.jsx)(e.code,{children:"navigate(kitchen)"})," \u2192 ",(0,s.jsx)(e.code,{children:"open(drawer)"})," \u2192 ",(0,s.jsx)(e.code,{children:"grasp(item)"})," \u2192 ",(0,s.jsx)(e.code,{children:"navigate(living_room)"})," \u2192 ",(0,s.jsx)(e.code,{children:"place(table)"})]}),"\n"]}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Implementation"})," (Behavior Tree):"]}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-python",children:"class BehaviorTree:\r\n    def __init__(self, vla_model):\r\n        self.vla = vla_model\r\n        self.skills = {\r\n            'grasp': self.grasp_skill,\r\n            'place': self.place_skill,\r\n            'navigate': self.navigate_skill,\r\n            'open': self.open_skill\r\n        }\r\n\r\n    def execute_task(self, task_description):\r\n        \"\"\"\r\n        Parse language instruction into skill sequence\r\n\r\n        Example:\r\n            \"Go to kitchen and pick up the mug\"\r\n            \u2192 ['navigate(kitchen)', 'grasp(mug)']\r\n        \"\"\"\r\n        skill_sequence = self.parse_to_skills(task_description)\r\n\r\n        for skill_name, args in skill_sequence:\r\n            skill_fn = self.skills[skill_name]\r\n            success = skill_fn(*args)\r\n\r\n            if not success:\r\n                return False  # Task failed\r\n\r\n        return True  # All skills succeeded\r\n\r\n    def parse_to_skills(self, instruction):\r\n        # Use VLA language understanding\r\n        # Or simple rule-based parsing for now\r\n        if \"go to\" in instruction and \"pick up\" in instruction:\r\n            location = extract_location(instruction)\r\n            object_name = extract_object(instruction)\r\n            return [('navigate', [location]), ('grasp', [object_name])]\r\n        # ... more rules\n"})}),"\n",(0,s.jsx)(e.h3,{id:"multi-task-vla-training",children:"Multi-Task VLA Training"}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Dataset"}),": Mix of manipulation, navigation, and tool-use tasks"]}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Training Strategy"}),":"]}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-python",children:"# Multi-task dataset\r\ndatasets = {\r\n    'manipulation': load_dataset('bridge_v2'),  # 60k demos\r\n    'navigation': load_dataset('go_stanford'),  # 50k demos\r\n    'tool_use': load_dataset('language_table')  # 40k demos\r\n}\r\n\r\n# Multi-task dataloader\r\nmulti_task_loader = MultiTaskDataLoader(datasets, batch_size=32)\r\n\r\n# Training loop\r\nfor batch in multi_task_loader:\r\n    images, instructions, actions, task_ids = batch\r\n\r\n    # Forward pass (task-conditioned)\r\n    predicted_actions = vla(images, instructions, task_id=task_ids)\r\n\r\n    # Compute loss (per-task weighting)\r\n    losses = {}\r\n    for task_name in ['manipulation', 'navigation', 'tool_use']:\r\n        mask = (task_ids == task_name)\r\n        if mask.sum() > 0:\r\n            losses[task_name] = F.mse_loss(\r\n                predicted_actions[mask],\r\n                actions[mask]\r\n            )\r\n\r\n    # Weighted combination\r\n    total_loss = 0.5 * losses['manipulation'] + \\\r\n                 0.3 * losses['navigation'] + \\\r\n                 0.2 * losses['tool_use']\r\n\r\n    total_loss.backward()\r\n    optimizer.step()\n"})}),"\n",(0,s.jsx)(e.h3,{id:"transfer-learning",children:"Transfer Learning"}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Scenario"}),": Train on simulated humanoid \u2192 Deploy on real humanoid"]}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Approach"}),":"]}),"\n",(0,s.jsxs)(e.ol,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Pretrain"})," on Open X-Embodiment (diverse robots, 1M demos)"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Fine-tune"})," on target humanoid in simulation (Isaac Sim, 10k demos)"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Domain adapt"})," with real robot data (1k demos)"]}),"\n"]}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-python",children:'# Step 1: Pretrain (done by OpenVLA team)\r\nvla = OpenVLA.from_pretrained("openvla-7b")\r\n\r\n# Step 2: Fine-tune on humanoid sim data\r\nhumanoid_sim_data = load_isaac_sim_dataset("humanoid_tasks")\r\nvla.fine_tune(humanoid_sim_data, epochs=20)\r\n\r\n# Step 3: Domain adaptation (real robot)\r\nreal_robot_data = collect_real_demos(num_demos=1000)\r\nvla.fine_tune(\r\n    real_robot_data,\r\n    epochs=5,\r\n    learning_rate=1e-6,  # Small LR to avoid catastrophic forgetting\r\n    freeze_encoder=True  # Only update action head\r\n)\r\n\r\n# Deploy\r\nvla.save("openvla-humanoid-real")\n'})}),"\n",(0,s.jsx)(e.h3,{id:"continual-learning",children:"Continual Learning"}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Problem"}),": Robot learns new task, forgets old tasks (catastrophic forgetting)"]}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Solution"}),": Elastic Weight Consolidation (EWC)"]}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-python",children:'class ContinualVLA:\r\n    def __init__(self, base_model):\r\n        self.model = base_model\r\n        self.fisher_information = {}  # Importance of each weight\r\n        self.optimal_params = {}  # Previous task\'s optimal weights\r\n\r\n    def train_new_task(self, new_task_data, lambda_ewc=1000):\r\n        """\r\n        Train on new task while preserving old task performance\r\n\r\n        Args:\r\n            new_task_data: Dataset for new task\r\n            lambda_ewc: Regularization strength\r\n        """\r\n        optimizer = torch.optim.Adam(self.model.parameters(), lr=1e-5)\r\n\r\n        for batch in new_task_data:\r\n            # Standard loss on new task\r\n            loss_new = self.model.compute_loss(batch)\r\n\r\n            # EWC penalty (preserve important weights)\r\n            loss_ewc = 0\r\n            for name, param in self.model.named_parameters():\r\n                if name in self.fisher_information:\r\n                    fisher = self.fisher_information[name]\r\n                    optimal = self.optimal_params[name]\r\n                    loss_ewc += (fisher * (param - optimal) ** 2).sum()\r\n\r\n            # Combined loss\r\n            loss = loss_new + lambda_ewc * loss_ewc\r\n\r\n            loss.backward()\r\n            optimizer.step()\r\n            optimizer.zero_grad()\r\n\r\n    def compute_fisher_information(self, old_task_data):\r\n        """\r\n        Compute Fisher information after training on a task\r\n        """\r\n        self.model.eval()\r\n        fisher = {n: torch.zeros_like(p) for n, p in self.model.named_parameters()}\r\n\r\n        for batch in old_task_data:\r\n            self.model.zero_grad()\r\n            loss = self.model.compute_loss(batch)\r\n            loss.backward()\r\n\r\n            for name, param in self.model.named_parameters():\r\n                fisher[name] += param.grad.data ** 2 / len(old_task_data)\r\n\r\n        self.fisher_information = fisher\r\n        self.optimal_params = {n: p.clone().detach() for n, p in self.model.named_parameters()}\n'})}),"\n",(0,s.jsx)(e.h2,{id:"exercises",children:"Exercises"}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Exercise 4.1"}),": VLA Grasping"]}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"Implement VLA-based grasping in simulation (Isaac Sim or Gazebo)"}),"\n",(0,s.jsx)(e.li,{children:"Test on 10 different objects (varied shapes, sizes, materials)"}),"\n",(0,s.jsx)(e.li,{children:"Measure success rate and analyze failure modes"}),"\n"]}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Exercise 4.2"}),": Whole-Body Manipulation"]}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"Train whole-body VLA with separate arm/leg heads"}),"\n",(0,s.jsx)(e.li,{children:'Task: "Walk forward 2 meters while holding a tray level"'}),"\n",(0,s.jsx)(e.li,{children:"Evaluate: walking speed, tray stability, success rate"}),"\n"]}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Exercise 4.3"}),": Multi-Task Learning"]}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"Create dataset with 3 tasks: grasping, navigation, opening drawers"}),"\n",(0,s.jsx)(e.li,{children:"Train single VLA on all tasks"}),"\n",(0,s.jsx)(e.li,{children:"Compare with 3 separate task-specific models (data efficiency, performance)"}),"\n"]}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Exercise 4.4"}),": Continual Learning"]}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"Train VLA on Task A (grasping cups)"}),"\n",(0,s.jsx)(e.li,{children:"Train on Task B (grasping boxes) with and without EWC"}),"\n",(0,s.jsx)(e.li,{children:"Measure forgetting: performance on Task A after learning Task B"}),"\n"]}),"\n",(0,s.jsx)(e.h2,{id:"summary",children:"Summary"}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Manipulation"}),": VLA for grasping, dexterous manipulation, tool use (affordance reasoning)\r\n",(0,s.jsx)(e.strong,{children:"Locomotion"}),": Hierarchical control (VLA high-level, RL low-level), whole-body coordination\r\n",(0,s.jsx)(e.strong,{children:"Multi-Task"}),": Skill composition, task-conditioned training, continual learning (EWC)"]}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Module 4 Complete!"})," Next module covers capstone project: autonomous humanoid system."]})]})}function p(n={}){const{wrapper:e}={...(0,o.R)(),...n.components};return e?(0,s.jsx)(e,{...n,children:(0,s.jsx)(d,{...n})}):d(n)}},28453:(n,e,r)=>{r.d(e,{R:()=>a,x:()=>t});var i=r(96540);const s={},o=i.createContext(s);function a(n){const e=i.useContext(o);return i.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function t(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(s):n.components||s:a(n.components),i.createElement(o.Provider,{value:e},n.children)}}}]);
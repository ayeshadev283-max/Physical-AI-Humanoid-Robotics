"use strict";(globalThis.webpackChunkphysical_ai_humanoid_robotics_book=globalThis.webpackChunkphysical_ai_humanoid_robotics_book||[]).push([[8227],{26626:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>l,contentTitle:()=>a,default:()=>h,frontMatter:()=>o,metadata:()=>s,toc:()=>c});const s=JSON.parse('{"id":"chapters/module-0-foundations/sensing-perception","title":"Chapter 3: Sensing and Perception","description":"Sensor modalities, fusion, and perception-action loops","source":"@site/docs/chapters/module-0-foundations/03-sensing-perception.md","sourceDirName":"chapters/module-0-foundations","slug":"/chapters/module-0-foundations/sensing-perception","permalink":"/Physical-AI-Humanoid-Robotics/chapters/module-0-foundations/sensing-perception","draft":false,"unlisted":false,"editUrl":"https://github.com/ayeshadev283-max/Physical-AI-Humanoid-Robotics/tree/main/docs/chapters/module-0-foundations/03-sensing-perception.md","tags":[{"inline":true,"label":"sensors","permalink":"/Physical-AI-Humanoid-Robotics/tags/sensors"},{"inline":true,"label":"perception","permalink":"/Physical-AI-Humanoid-Robotics/tags/perception"},{"inline":true,"label":"sensor-fusion","permalink":"/Physical-AI-Humanoid-Robotics/tags/sensor-fusion"},{"inline":true,"label":"proprioception","permalink":"/Physical-AI-Humanoid-Robotics/tags/proprioception"}],"version":"current","sidebarPosition":3,"frontMatter":{"sidebar_position":3,"title":"Chapter 3: Sensing and Perception","description":"Sensor modalities, fusion, and perception-action loops","tags":["sensors","perception","sensor-fusion","proprioception"]},"sidebar":"bookSidebar","previous":{"title":"Chapter 2: Embodied Intelligence","permalink":"/Physical-AI-Humanoid-Robotics/chapters/module-0-foundations/embodied-intelligence"},"next":{"title":"Chapter 4: Locomotion and Motor Control","permalink":"/Physical-AI-Humanoid-Robotics/chapters/module-0-foundations/locomotion-motor-control"}}');var r=i(74848),t=i(28453);const o={sidebar_position:3,title:"Chapter 3: Sensing and Perception",description:"Sensor modalities, fusion, and perception-action loops",tags:["sensors","perception","sensor-fusion","proprioception"]},a="Chapter 3: Sensing and Perception",l={},c=[{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"3.1 Sensor Modalities",id:"31-sensor-modalities",level:2},{value:"Vision: Cameras",id:"vision-cameras",level:3},{value:"Ranging: LiDAR",id:"ranging-lidar",level:3},{value:"Inertial: IMU",id:"inertial-imu",level:3},{value:"Proprioception",id:"proprioception",level:3},{value:"Tactile",id:"tactile",level:3},{value:"3.2 Sensor Fusion Architectures",id:"32-sensor-fusion-architectures",level:2},{value:"Kalman Filter",id:"kalman-filter",level:3},{value:"Particle Filter",id:"particle-filter",level:3},{value:"Modern Approach: Learned Fusion",id:"modern-approach-learned-fusion",level:3},{value:"3.3 Perception-Action Loop",id:"33-perception-action-loop",level:2},{value:"Active Perception",id:"active-perception",level:3},{value:"Sensorimotor Contingencies",id:"sensorimotor-contingencies",level:3},{value:"Real-Time Constraints",id:"real-time-constraints",level:3},{value:"Summary",id:"summary",level:2},{value:"Exercises",id:"exercises",level:2}];function d(e){const n={annotation:"annotation",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",math:"math",mermaid:"mermaid",mi:"mi",mn:"mn",mo:"mo",mrow:"mrow",ol:"ol",p:"p",pre:"pre",semantics:"semantics",span:"span",strong:"strong",ul:"ul",...(0,t.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.header,{children:(0,r.jsx)(n.h1,{id:"chapter-3-sensing-and-perception",children:"Chapter 3: Sensing and Perception"})}),"\n",(0,r.jsx)(n.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsx)(n.li,{children:"Identify key sensor modalities and their trade-offs"}),"\n",(0,r.jsx)(n.li,{children:"Explain sensor fusion architectures"}),"\n",(0,r.jsx)(n.li,{children:"Understand the perception-action loop"}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"31-sensor-modalities",children:"3.1 Sensor Modalities"}),"\n",(0,r.jsx)(n.h3,{id:"vision-cameras",children:"Vision: Cameras"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"RGB Cameras"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Rich semantic information (colors, textures, text)"}),"\n",(0,r.jsxs)(n.li,{children:["Cheap (",(0,r.jsxs)(n.span,{className:"katex",children:[(0,r.jsx)(n.span,{className:"katex-mathml",children:(0,r.jsx)(n.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,r.jsxs)(n.semantics,{children:[(0,r.jsxs)(n.mrow,{children:[(0,r.jsx)(n.mn,{children:"10"}),(0,r.jsx)(n.mo,{children:"\u2212"})]}),(0,r.jsx)(n.annotation,{encoding:"application/x-tex",children:"10-"})]})})}),(0,r.jsx)(n.span,{className:"katex-html","aria-hidden":"true",children:(0,r.jsxs)(n.span,{className:"base",children:[(0,r.jsx)(n.span,{className:"strut",style:{height:"0.7278em",verticalAlign:"-0.0833em"}}),(0,r.jsx)(n.span,{className:"mord",children:"10"}),(0,r.jsx)(n.span,{className:"mord",children:"\u2212"})]})})]}),"100)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Limitations"}),": No direct depth, lighting-dependent"]}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Depth Cameras"})," (RealSense, Kinect):"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Stereo or structured light for 3D"}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Trade-off"}),": Indoor only, limited range (0.5-5m)"]}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Event Cameras"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Asynchronous pixels (fire on brightness change)"}),"\n",(0,r.jsx)(n.li,{children:"High temporal resolution (microseconds)"}),"\n",(0,r.jsx)(n.li,{children:"Low latency for fast motion"}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"ranging-lidar",children:"Ranging: LiDAR"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Principle"}),": Time-of-flight laser measurement"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Advantages"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Accurate 3D (mm precision)"}),"\n",(0,r.jsx)(n.li,{children:"Works in dark, immune to lighting"}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Disadvantages"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["Expensive (",(0,r.jsxs)(n.span,{className:"katex",children:[(0,r.jsx)(n.span,{className:"katex-mathml",children:(0,r.jsx)(n.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,r.jsxs)(n.semantics,{children:[(0,r.jsxs)(n.mrow,{children:[(0,r.jsx)(n.mn,{children:"1"}),(0,r.jsx)(n.mi,{children:"k"}),(0,r.jsx)(n.mo,{children:"\u2212"})]}),(0,r.jsx)(n.annotation,{encoding:"application/x-tex",children:"1k-"})]})})}),(0,r.jsx)(n.span,{className:"katex-html","aria-hidden":"true",children:(0,r.jsxs)(n.span,{className:"base",children:[(0,r.jsx)(n.span,{className:"strut",style:{height:"0.7778em",verticalAlign:"-0.0833em"}}),(0,r.jsx)(n.span,{className:"mord",children:"1"}),(0,r.jsx)(n.span,{className:"mord mathnormal",style:{marginRight:"0.03148em"},children:"k"}),(0,r.jsx)(n.span,{className:"mord",children:"\u2212"})]})})]}),"10k)"]}),"\n",(0,r.jsx)(n.li,{children:"Reflective/transparent surfaces fail"}),"\n",(0,r.jsx)(n.li,{children:"Moving parts (mechanical LiDAR)"}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"inertial-imu",children:"Inertial: IMU"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Components"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Accelerometer: Linear acceleration (m/s\xb2)"}),"\n",(0,r.jsx)(n.li,{children:"Gyroscope: Angular velocity (rad/s)"}),"\n",(0,r.jsx)(n.li,{children:"Magnetometer: Orientation (compass)"}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Use Cases"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Balance control (detect tipping)"}),"\n",(0,r.jsx)(n.li,{children:"Dead reckoning (integrate acceleration \u2192 velocity \u2192 position)"}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Drift Problem"}),": Integration error accumulates"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"proprioception",children:"Proprioception"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Joint Encoders"}),": Measure joint angles (0.01\xb0 resolution)\r\n",(0,r.jsx)(n.strong,{children:"Force/Torque Sensors"}),": Detect contact, measure grip strength"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Importance"}),": Know body state independent of vision."]}),"\n",(0,r.jsx)(n.h3,{id:"tactile",children:"Tactile"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Types"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Resistive (pressure changes resistance)"}),"\n",(0,r.jsx)(n.li,{children:"Capacitive (proximity/contact)"}),"\n",(0,r.jsx)(n.li,{children:"Optical (camera inside soft skin)"}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Applications"}),": Grasp stability, texture recognition, safe human contact."]}),"\n",(0,r.jsx)(n.h2,{id:"32-sensor-fusion-architectures",children:"3.2 Sensor Fusion Architectures"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Problem"}),": Each sensor has errors, blind spots, delays."]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Solution"}),": Combine redundant sensors for robustness."]}),"\n",(0,r.jsx)(n.mermaid,{value:'graph TB\r\n    subgraph Sensors["Sensor Array"]\r\n        GPS[GPS<br/>Position<br/>Slow, Accurate]\r\n        IMU[IMU<br/>Acceleration<br/>Fast, Drifts]\r\n        Camera[Camera<br/>Visual Features<br/>Rich, Noisy]\r\n        Lidar[LiDAR<br/>3D Points<br/>Precise, Expensive]\r\n    end\r\n\r\n    subgraph Fusion["Sensor Fusion (Kalman Filter)"]\r\n        Predict[Prediction Step<br/>Use IMU for fast update]\r\n        Update[Update Step<br/>Correct with GPS/Camera/LiDAR]\r\n        Predict --\x3e Update\r\n        Update --\x3e Predict\r\n    end\r\n\r\n    subgraph Output["Fused Estimate"]\r\n        State[Robot State<br/>Position, Velocity, Orientation<br/>Fast + Accurate]\r\n    end\r\n\r\n    GPS --\x3e Update\r\n    IMU --\x3e Predict\r\n    Camera --\x3e Update\r\n    Lidar --\x3e Update\r\n    Update --\x3e State\r\n\r\n    style Sensors fill:#ffe1cc\r\n    style Fusion fill:#cce1ff\r\n    style Output fill:#ccffcc'}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Figure 3.1"}),": Sensor fusion pipeline combining complementary sensors (GPS, IMU, camera, LiDAR) using a Kalman filter to produce a fast and accurate state estimate."]}),"\n",(0,r.jsx)(n.h3,{id:"kalman-filter",children:"Kalman Filter"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Use Case"}),": Fusing GPS + IMU for position"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Idea"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Prediction: Use IMU to predict position (fast, drifts)"}),"\n",(0,r.jsx)(n.li,{children:"Update: Correct with GPS (slow, accurate)"}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Result"}),": Fast + accurate estimate"]}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Formula"})," (simplified):"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"x\u0302 = Prediction + K \xd7 (Measurement - Prediction)\n"})}),"\n",(0,r.jsx)(n.p,{children:"K = Kalman Gain (how much to trust measurement vs prediction)"}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Pseudocode Example: Kalman Filter for GPS + IMU Fusion"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"# State: [position_x, position_y, velocity_x, velocity_y]\r\n# Measurements: GPS (position), IMU (acceleration)\r\n\r\ndef kalman_filter(state, covariance, measurement, dt):\r\n    # Prediction step (use IMU)\r\n    F = [[1, 0, dt, 0],   # State transition matrix\r\n         [0, 1, 0, dt],\r\n         [0, 0, 1, 0],\r\n         [0, 0, 0, 1]]\r\n    \r\n    predicted_state = F @ state + B @ imu_acceleration\r\n    predicted_cov = F @ covariance @ F.T + process_noise\r\n    \r\n    # Update step (use GPS)\r\n    H = [[1, 0, 0, 0],    # Measurement matrix (GPS measures position only)\r\n         [0, 1, 0, 0]]\r\n    \r\n    innovation = gps_measurement - H @ predicted_state\r\n    kalman_gain = predicted_cov @ H.T @ inv(H @ predicted_cov @ H.T + measurement_noise)\r\n    \r\n    updated_state = predicted_state + kalman_gain @ innovation\r\n    updated_cov = (I - kalman_gain @ H) @ predicted_cov\r\n    \r\n    return updated_state, updated_cov\n"})}),"\n",(0,r.jsx)(n.p,{children:"This combines fast IMU predictions with slower but accurate GPS corrections."}),"\n",(0,r.jsx)(n.h3,{id:"particle-filter",children:"Particle Filter"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Use Case"}),": Robot localization with unknown position"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Idea"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Represent belief as particles (samples)"}),"\n",(0,r.jsx)(n.li,{children:"Each particle = possible robot pose"}),"\n",(0,r.jsx)(n.li,{children:"Resample based on sensor likelihood"}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Handles"}),": Multi-modal distributions (multiple hypotheses)"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"modern-approach-learned-fusion",children:"Modern Approach: Learned Fusion"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Neural Networks"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Input: Multi-modal data (image + LiDAR + IMU)"}),"\n",(0,r.jsx)(n.li,{children:"Output: Fused representation (e.g., occupancy grid)"}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Advantage"}),": Learns sensor correlations from data"]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"33-perception-action-loop",children:"3.3 Perception-Action Loop"}),"\n",(0,r.jsx)(n.mermaid,{value:'graph LR\r\n    subgraph World["Physical Environment"]\r\n        State[Environmental State]\r\n    end\r\n\r\n    subgraph Robot["Robot System"]\r\n        Sensors[Sensors]\r\n        Perception[Perception<br/>State Estimation]\r\n        Planning[Action Selection<br/>Planning/Control]\r\n        Actuators[Actuators]\r\n    end\r\n\r\n    State --\x3e|Sense| Sensors\r\n    Sensors --\x3e|Data| Perception\r\n    Perception --\x3e|World Model| Planning\r\n    Planning --\x3e|Commands| Actuators\r\n    Actuators --\x3e|Action| State\r\n    \r\n    Planning -.->|Active Perception<br/>Move to improve sensing| Actuators\r\n\r\n    style World fill:#fff4e1\r\n    style Robot fill:#e1f5ff'}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Figure 3.2"}),": The perception-action loop showing how action affects perception (active perception) and how sensory feedback continuously guides action. This is a continuous cycle, not a sequential pipeline."]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Classical View"}),": Sense \u2192 Perceive \u2192 Plan \u2192 Act (sequential)"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Embodied View"}),": Continuous loop, action affects perception"]}),"\n",(0,r.jsx)(n.h3,{id:"active-perception",children:"Active Perception"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Concept"}),": Move to improve sensing"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Examples"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Turn head to see occluded object"}),"\n",(0,r.jsx)(n.li,{children:"Poke object to infer mass/compliance"}),"\n",(0,r.jsx)(n.li,{children:"Shake box to hear contents"}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"sensorimotor-contingencies",children:"Sensorimotor Contingencies"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Definition"}),": Learned relationships between actions and sensory changes"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Robot Example"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Move forward \u2192 objects grow in image"}),"\n",(0,r.jsx)(n.li,{children:"Turn left \u2192 visual flow rightward"}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Learning"}),": Build model of these contingencies"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"real-time-constraints",children:"Real-Time Constraints"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Humanoid Walking"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Sense: IMU at 1kHz, cameras at 30Hz"}),"\n",(0,r.jsx)(n.li,{children:"Control: Joint commands at 100Hz"}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Challenge"}),": Fuse asynchronous sensors for real-time control"]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Sensors"}),": Vision, LiDAR, IMU, proprioception, tactile\r\n",(0,r.jsx)(n.strong,{children:"Fusion"}),": Kalman, particle filters, learned fusion\r\n",(0,r.jsx)(n.strong,{children:"Loop"}),": Perception and action are coupled, not sequential"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Next"}),": Chapter 4 on locomotion and motor control."]}),"\n",(0,r.jsx)(n.h2,{id:"exercises",children:"Exercises"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Exercise 3.1"}),": Choose two sensor modalities from Section 3.1 and design a sensor fusion approach for a specific task (e.g., autonomous navigation, grasping). Justify why combining these sensors improves performance over using either alone."]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Exercise 3.2"}),": Implement the Kalman filter pseudocode in Python or your preferred language. Test it with simulated GPS and IMU data where the IMU has high-frequency noise and the GPS has low-frequency drift."]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Exercise 3.3"}),': Give three examples of "active perception" in everyday human activities (beyond the examples in this chapter). For each, explain how action improves sensing and what would be lost if the system were purely passive.']})]})}function h(e={}){const{wrapper:n}={...(0,t.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(d,{...e})}):d(e)}},28453:(e,n,i)=>{i.d(n,{R:()=>o,x:()=>a});var s=i(96540);const r={},t=s.createContext(r);function o(e){const n=s.useContext(t);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:o(e.components),s.createElement(t.Provider,{value:n},e.children)}}}]);
"use strict";(globalThis.webpackChunkphysical_ai_humanoid_robotics_book=globalThis.webpackChunkphysical_ai_humanoid_robotics_book||[]).push([[9707],{28453:(e,n,r)=>{r.d(n,{R:()=>o,x:()=>a});var t=r(96540);const s={},i=t.createContext(s);function o(e){const n=t.useContext(i);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:o(e.components),t.createElement(i.Provider,{value:n},e.children)}},65828:(e,n,r)=>{r.r(n),r.d(n,{assets:()=>l,contentTitle:()=>a,default:()=>p,frontMatter:()=>o,metadata:()=>t,toc:()=>c});const t=JSON.parse('{"id":"chapters/module-5-capstone/vla-autonomy","title":"Chapter 4: VLA Autonomy","description":"VLA policy integration, task planning, and real-world deployment for autonomous humanoids","source":"@site/docs/chapters/module-5-capstone/04-vla-autonomy.md","sourceDirName":"chapters/module-5-capstone","slug":"/chapters/module-5-capstone/vla-autonomy","permalink":"/Physical-AI-Humanoid-Robotics/docs/chapters/module-5-capstone/vla-autonomy","draft":false,"unlisted":false,"editUrl":"https://github.com/ayeshadev283-max/Physical-AI-Humanoid-Robotics/tree/main/docs/chapters/module-5-capstone/04-vla-autonomy.md","tags":[{"inline":true,"label":"vla","permalink":"/Physical-AI-Humanoid-Robotics/docs/tags/vla"},{"inline":true,"label":"autonomy","permalink":"/Physical-AI-Humanoid-Robotics/docs/tags/autonomy"},{"inline":true,"label":"task-planning","permalink":"/Physical-AI-Humanoid-Robotics/docs/tags/task-planning"},{"inline":true,"label":"deployment","permalink":"/Physical-AI-Humanoid-Robotics/docs/tags/deployment"},{"inline":true,"label":"testing","permalink":"/Physical-AI-Humanoid-Robotics/docs/tags/testing"}],"version":"current","sidebarPosition":4,"frontMatter":{"sidebar_position":4,"title":"Chapter 4: VLA Autonomy","description":"VLA policy integration, task planning, and real-world deployment for autonomous humanoids","tags":["vla","autonomy","task-planning","deployment","testing"]},"sidebar":"bookSidebar","previous":{"title":"Chapter 3: Control Stack","permalink":"/Physical-AI-Humanoid-Robotics/docs/chapters/module-5-capstone/control-stack"},"next":{"title":"Appendices","permalink":"/Physical-AI-Humanoid-Robotics/docs/appendices"}}');var s=r(74848),i=r(28453);const o={sidebar_position:4,title:"Chapter 4: VLA Autonomy",description:"VLA policy integration, task planning, and real-world deployment for autonomous humanoids",tags:["vla","autonomy","task-planning","deployment","testing"]},a="Chapter 4: VLA Autonomy",l={},c=[{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"4.1 VLA Policy Integration",id:"41-vla-policy-integration",level:2},{value:"OpenVLA Deployment on Humanoid",id:"openvla-deployment-on-humanoid",level:3},{value:"Fine-Tuning OpenVLA for Humanoid",id:"fine-tuning-openvla-for-humanoid",level:3},{value:"ROS 2 Integration",id:"ros-2-integration",level:3},{value:"4.2 Task Planning and Execution",id:"42-task-planning-and-execution",level:2},{value:"Behavior Tree Framework",id:"behavior-tree-framework",level:3},{value:"Error Recovery",id:"error-recovery",level:3},{value:"4.3 Real-World Testing and Iteration",id:"43-real-world-testing-and-iteration",level:2},{value:"Deployment Checklist",id:"deployment-checklist",level:3},{value:"Evaluation Metrics",id:"evaluation-metrics",level:3},{value:"Iterative Improvement",id:"iterative-improvement",level:3},{value:"Exercises",id:"exercises",level:2},{value:"Summary",id:"summary",level:2},{value:"Capstone Project Checklist",id:"capstone-project-checklist",level:2}];function d(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",mermaid:"mermaid",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,i.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"chapter-4-vla-autonomy",children:"Chapter 4: VLA Autonomy"})}),"\n",(0,s.jsx)(n.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsx)(n.li,{children:"Integrate OpenVLA policy with humanoid control stack"}),"\n",(0,s.jsx)(n.li,{children:"Implement task planning and execution framework"}),"\n",(0,s.jsx)(n.li,{children:"Deploy and test autonomous system in real-world scenarios"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"41-vla-policy-integration",children:"4.1 VLA Policy Integration"}),"\n",(0,s.jsx)(n.h3,{id:"openvla-deployment-on-humanoid",children:"OpenVLA Deployment on Humanoid"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"System Architecture"}),":"]}),"\n",(0,s.jsx)(n.mermaid,{value:"graph TB\r\n    User[User Command<br/>fetch the cup]\r\n\r\n    User --\x3e VLA[VLA Policy<br/>OpenVLA 7B]\r\n\r\n    Camera[RGB-D Camera] --\x3e VLA\r\n    State[Robot State<br/>joint angles, IMU] --\x3e VLA\r\n\r\n    VLA --\x3e Skills[Skill Library]\r\n\r\n    Skills --\x3e Navigate[navigate<br/>goal]\r\n    Skills --\x3e Grasp[grasp<br/>object]\r\n    Skills --\x3e Place[place<br/>location]\r\n    Skills --\x3e Open[open<br/>container]\r\n\r\n    Navigate --\x3e PathPlan[Path Planner<br/>Nav2]\r\n    Grasp --\x3e MotionPlan[Motion Planner<br/>cuRobo]\r\n    Place --\x3e MotionPlan\r\n    Open --\x3e MotionPlan\r\n\r\n    PathPlan --\x3e WBC[Whole-Body<br/>Controller]\r\n    MotionPlan --\x3e WBC\r\n\r\n    WBC --\x3e Robot[Robot<br/>Execution]\r\n\r\n    style VLA fill:#e1f5ff\r\n    style Skills fill:#ffffcc\r\n    style WBC fill:#ffe1e1"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Figure 4.1"}),": VLA autonomy architecture showing language commands flowing through VLA policy to skill primitives, then to motion/path planners, and finally whole-body controller."]}),"\n",(0,s.jsx)(n.h3,{id:"fine-tuning-openvla-for-humanoid",children:"Fine-Tuning OpenVLA for Humanoid"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Dataset Collection"})," (simulation):"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"import isaac_sim\r\nfrom openvla.data import RobotDataset\r\n\r\nclass HumanoidDataCollector:\r\n    def __init__(self, sim_env):\r\n        self.env = sim_env\r\n        self.demonstrations = []\r\n\r\n    def collect_demonstration(self, task_instruction):\r\n        \"\"\"\r\n        Collect one demonstration via teleoperation or scripted policy\r\n\r\n        Args:\r\n            task_instruction: str, e.g., \"pick up the red cup\"\r\n\r\n        Returns:\r\n            demo: Dict with observations, actions, language\r\n        \"\"\"\r\n        demo = {\r\n            'images': [],\r\n            'joint_states': [],\r\n            'actions': [],\r\n            'instruction': task_instruction\r\n        }\r\n\r\n        # Reset environment\r\n        obs = self.env.reset()\r\n\r\n        # Execute task (teleoperation or expert policy)\r\n        done = False\r\n        while not done:\r\n            # Capture observation\r\n            image = self.env.get_camera_image()\r\n            joint_state = self.env.get_joint_positions()\r\n\r\n            # Get action (from human or expert)\r\n            action = self.get_expert_action()  # Implement this\r\n\r\n            # Execute\r\n            obs, reward, done, info = self.env.step(action)\r\n\r\n            # Store\r\n            demo['images'].append(image)\r\n            demo['joint_states'].append(joint_state)\r\n            demo['actions'].append(action)\r\n\r\n        self.demonstrations.append(demo)\r\n        return demo\r\n\r\n    def save_dataset(self, path):\r\n        \"\"\"Save demonstrations in RLDS format\"\"\"\r\n        dataset = RobotDataset(demonstrations=self.demonstrations)\r\n        dataset.save(path)\n"})}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Fine-Tuning"}),":"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'from openvla import OpenVLA\r\nfrom torch.utils.data import DataLoader\r\n\r\n# Load pretrained model\r\nvla = OpenVLA.from_pretrained("openvla-7b")\r\n\r\n# Load humanoid dataset\r\ndataset = RobotDataset(data_path="humanoid_demos/")\r\ndataloader = DataLoader(dataset, batch_size=16, shuffle=True)\r\n\r\n# Fine-tune\r\noptimizer = torch.optim.AdamW(vla.parameters(), lr=1e-5)\r\n\r\nfor epoch in range(20):\r\n    for batch in dataloader:\r\n        images, instructions, actions = batch\r\n\r\n        # Forward pass\r\n        predicted_actions = vla(images, instructions)\r\n        loss = F.mse_loss(predicted_actions, actions)\r\n\r\n        # Backward pass\r\n        loss.backward()\r\n        optimizer.step()\r\n        optimizer.zero_grad()\r\n\r\n    print(f"Epoch {epoch}, Loss: {loss.item():.4f}")\r\n\r\n# Save fine-tuned model\r\nvla.save_pretrained("openvla-humanoid")\n'})}),"\n",(0,s.jsx)(n.h3,{id:"ros-2-integration",children:"ROS 2 Integration"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"VLA Policy Node"})," (from Module 4, adapted for humanoid):"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"import rclpy\r\nfrom rclpy.node import Node\r\nfrom sensor_msgs.msg import Image, JointState\r\nfrom std_msgs.msg import String\r\nfrom trajectory_msgs.msg import JointTrajectory\r\nfrom cv_bridge import CvBridge\r\nfrom openvla import OpenVLA\r\n\r\nclass HumanoidVLANode(Node):\r\n    def __init__(self):\r\n        super().__init__('humanoid_vla_node')\r\n\r\n        # Load fine-tuned model\r\n        self.vla = OpenVLA.from_pretrained(\"openvla-humanoid\").to(\"cuda\")\r\n        self.vla.eval()\r\n\r\n        self.bridge = CvBridge()\r\n        self.current_image = None\r\n        self.current_joint_state = None\r\n        self.current_instruction = \"stand still\"\r\n\r\n        # Subscribers\r\n        self.image_sub = self.create_subscription(\r\n            Image, '/camera/color/image_raw', self.image_callback, 10\r\n        )\r\n        self.joint_sub = self.create_subscription(\r\n            JointState, '/joint_states', self.joint_callback, 10\r\n        )\r\n        self.instruction_sub = self.create_subscription(\r\n            String, '/task/instruction', self.instruction_callback, 10\r\n        )\r\n\r\n        # Publishers (high-level skills)\r\n        self.skill_pub = self.create_publisher(\r\n            String, '/vla/skill_command', 10\r\n        )\r\n\r\n        # Policy loop\r\n        self.timer = self.create_timer(0.1, self.policy_loop)  # 10 Hz\r\n\r\n    def policy_loop(self):\r\n        if self.current_image is None or self.current_joint_state is None:\r\n            return\r\n\r\n        # Predict skill from VLA\r\n        with torch.no_grad():\r\n            skill_command = self.vla.predict_skill(\r\n                image=self.current_image,\r\n                instruction=self.current_instruction,\r\n                proprioception=self.current_joint_state\r\n            )\r\n\r\n        # Publish skill (e.g., \"navigate(kitchen)\" or \"grasp(cup)\")\r\n        msg = String()\r\n        msg.data = skill_command\r\n        self.skill_pub.publish(msg)\r\n\r\n    def image_callback(self, msg):\r\n        self.current_image = self.bridge.imgmsg_to_cv2(msg, 'rgb8')\r\n\r\n    def joint_callback(self, msg):\r\n        self.current_joint_state = np.array(msg.position)\r\n\r\n    def instruction_callback(self, msg):\r\n        self.current_instruction = msg.data\r\n        self.get_logger().info(f\"New instruction: {msg.data}\")\n"})}),"\n",(0,s.jsx)(n.h2,{id:"42-task-planning-and-execution",children:"4.2 Task Planning and Execution"}),"\n",(0,s.jsx)(n.h3,{id:"behavior-tree-framework",children:"Behavior Tree Framework"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Skill Primitives"}),":"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'import py_trees\r\n\r\nclass NavigateSkill(py_trees.behaviour.Behaviour):\r\n    def __init__(self, goal_location):\r\n        super().__init__(name=f"Navigate to {goal_location}")\r\n        self.goal = goal_location\r\n\r\n    def update(self):\r\n        # Publish navigation goal to Nav2\r\n        self.publish_nav_goal(self.goal)\r\n\r\n        # Check if reached\r\n        if self.distance_to_goal() < 0.5:  # meters\r\n            return py_trees.common.Status.SUCCESS\r\n        else:\r\n            return py_trees.common.Status.RUNNING\r\n\r\nclass GraspSkill(py_trees.behaviour.Behaviour):\r\n    def __init__(self, object_name):\r\n        super().__init__(name=f"Grasp {object_name}")\r\n        self.object = object_name\r\n\r\n    def update(self):\r\n        # Detect object\r\n        object_pose = self.detect_object(self.object)\r\n\r\n        if object_pose is None:\r\n            return py_trees.common.Status.FAILURE\r\n\r\n        # Plan grasp\r\n        grasp_trajectory = self.plan_grasp(object_pose)\r\n\r\n        # Execute\r\n        self.execute_trajectory(grasp_trajectory)\r\n\r\n        # Check success (force sensor)\r\n        if self.gripper_force() > 1.0:\r\n            return py_trees.common.Status.SUCCESS\r\n        else:\r\n            return py_trees.common.Status.FAILURE\n'})}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Composing Tasks"}),":"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'def create_fetch_task(object_name, delivery_location):\r\n    """\r\n    Create behavior tree for fetch task\r\n\r\n    Task: "Fetch me the {object_name} from the {source_location}"\r\n    """\r\n    root = py_trees.composites.Sequence(name="Fetch Task")\r\n\r\n    # 1. Navigate to object\r\n    navigate_to_object = NavigateSkill(goal_location="kitchen")\r\n    root.add_child(navigate_to_object)\r\n\r\n    # 2. Grasp object\r\n    grasp_object = GraspSkill(object_name=object_name)\r\n    root.add_child(grasp_object)\r\n\r\n    # 3. Navigate to user\r\n    navigate_to_user = NavigateSkill(goal_location=delivery_location)\r\n    root.add_child(navigate_to_user)\r\n\r\n    # 4. Hand over (open gripper)\r\n    handover = HandoverSkill()\r\n    root.add_child(handover)\r\n\r\n    return root\r\n\r\n# Execute\r\ntask_tree = create_fetch_task("cup", "living_room")\r\ntask_tree.setup_with_descendants()\r\n\r\nwhile task_tree.status != py_trees.common.Status.SUCCESS:\r\n    task_tree.tick_once()\r\n    time.sleep(0.1)\r\n\r\nprint("Task completed!")\n'})}),"\n",(0,s.jsx)(n.h3,{id:"error-recovery",children:"Error Recovery"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Retry Logic"}),":"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'class RobustGraspSkill(GraspSkill):\r\n    def __init__(self, object_name, max_retries=3):\r\n        super().__init__(object_name)\r\n        self.max_retries = max_retries\r\n        self.retry_count = 0\r\n\r\n    def update(self):\r\n        # Attempt grasp\r\n        result = super().update()\r\n\r\n        if result == py_trees.common.Status.FAILURE:\r\n            self.retry_count += 1\r\n\r\n            if self.retry_count < self.max_retries:\r\n                self.get_logger().warn(f"Grasp failed, retrying ({self.retry_count}/{self.max_retries})")\r\n                # Adjust grasp pose slightly\r\n                self.adjust_grasp_strategy()\r\n                return py_trees.common.Status.RUNNING\r\n            else:\r\n                self.get_logger().error("Max retries exceeded")\r\n                return py_trees.common.Status.FAILURE\r\n\r\n        return result\n'})}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Fallback Behaviors"}),":"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'fallback = py_trees.composites.Selector(name="Grasp with Fallback")\r\n\r\n# Try primary grasp\r\nfallback.add_child(GraspSkill("cup"))\r\n\r\n# Fallback 1: Try different approach angle\r\nfallback.add_child(GraspSkill("cup", approach="side"))\r\n\r\n# Fallback 2: Request human help\r\nfallback.add_child(RequestHumanHelp("Cannot grasp cup"))\r\n\r\nfallback.tick_once()\n'})}),"\n",(0,s.jsx)(n.h2,{id:"43-real-world-testing-and-iteration",children:"4.3 Real-World Testing and Iteration"}),"\n",(0,s.jsx)(n.h3,{id:"deployment-checklist",children:"Deployment Checklist"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Pre-Deployment"}),":"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"\u2705 Simulate 100+ task executions (80%+ success)"}),"\n",(0,s.jsx)(n.li,{children:"\u2705 Safety tests (emergency stop, collision avoidance, fall detection)"}),"\n",(0,s.jsx)(n.li,{children:"\u2705 Battery life test (2+ hours continuous operation)"}),"\n",(0,s.jsxs)(n.li,{children:["\u2705 Latency verification (VLA policy ",(0,s.jsx)(n.code,{children:"<50ms"}),", end-to-end ",(0,s.jsx)(n.code,{children:"<10s"}),")"]}),"\n",(0,s.jsx)(n.li,{children:"\u2705 Hardware checks (all sensors functional, motors calibrated)"}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Initial Deployment"})," (controlled environment):"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"\u2705 Human supervisor present at all times"}),"\n",(0,s.jsx)(n.li,{children:"\u2705 Soft objects only (foam, cloth) for first 50 trials"}),"\n",(0,s.jsx)(n.li,{children:"\u2705 Restricted workspace (1m radius)"}),"\n",(0,s.jsx)(n.li,{children:"\u2705 Emergency stop button within reach"}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"evaluation-metrics",children:"Evaluation Metrics"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Task Success Rate"}),":"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"class TaskEvaluator:\r\n    def __init__(self):\r\n        self.results = []\r\n\r\n    def evaluate_task(self, task_name, success, time_taken, error_type=None):\r\n        \"\"\"\r\n        Log task execution result\r\n\r\n        Args:\r\n            task_name: str\r\n            success: bool\r\n            time_taken: float (seconds)\r\n            error_type: str or None (e.g., \"grasp_failure\", \"navigation_timeout\")\r\n        \"\"\"\r\n        self.results.append({\r\n            'task': task_name,\r\n            'success': success,\r\n            'time': time_taken,\r\n            'error': error_type\r\n        })\r\n\r\n    def compute_metrics(self):\r\n        \"\"\"\r\n        Compute aggregate metrics\r\n\r\n        Returns:\r\n            dict: Success rate, avg time, error breakdown\r\n        \"\"\"\r\n        total = len(self.results)\r\n        successes = sum(r['success'] for r in self.results)\r\n\r\n        metrics = {\r\n            'success_rate': successes / total,\r\n            'avg_time': np.mean([r['time'] for r in self.results if r['success']]),\r\n            'error_breakdown': {}\r\n        }\r\n\r\n        # Error analysis\r\n        for result in self.results:\r\n            if result['error'] is not None:\r\n                error_type = result['error']\r\n                metrics['error_breakdown'][error_type] = \\\r\n                    metrics['error_breakdown'].get(error_type, 0) + 1\r\n\r\n        return metrics\r\n\r\n# Usage\r\nevaluator = TaskEvaluator()\r\n\r\nfor i in range(100):\r\n    success, time_taken, error = execute_fetch_task(\"cup\")\r\n    evaluator.evaluate_task(\"fetch_cup\", success, time_taken, error)\r\n\r\nmetrics = evaluator.compute_metrics()\r\nprint(f\"Success rate: {metrics['success_rate']:.1%}\")\r\nprint(f\"Average time: {metrics['avg_time']:.1f}s\")\r\nprint(f\"Errors: {metrics['error_breakdown']}\")\n"})}),"\n",(0,s.jsx)(n.h3,{id:"iterative-improvement",children:"Iterative Improvement"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Failure Analysis"}),":"]}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Collect failure cases"})," (video + sensor logs)"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Categorize errors"})," (perception, planning, control, hardware)"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Prioritize fixes"})," (highest impact, easiest to fix)"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Update system"})," (retrain VLA, tune controllers, fix bugs)"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Re-evaluate"})," (measure improvement)"]}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"DAgger for Continuous Learning"}),":"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"class DAggerCollector:\r\n    def __init__(self, vla_policy, expert_policy):\r\n        self.vla = vla_policy\r\n        self.expert = expert_policy\r\n        self.new_demos = []\r\n\r\n    def collect_corrective_demo(self, task):\r\n        \"\"\"\r\n        Collect demonstration where VLA policy is corrected by expert\r\n\r\n        Args:\r\n            task: Task description\r\n\r\n        Returns:\r\n            demo: Demonstration with VLA actions + expert corrections\r\n        \"\"\"\r\n        demo = {'images': [], 'actions_vla': [], 'actions_expert': []}\r\n\r\n        obs = self.env.reset()\r\n        done = False\r\n\r\n        while not done:\r\n            image = self.env.get_camera_image()\r\n\r\n            # VLA prediction\r\n            action_vla = self.vla.predict_action(image, task)\r\n\r\n            # Expert correction (human or oracle)\r\n            action_expert = self.expert.get_action(image, task)\r\n\r\n            # Execute expert action\r\n            obs, _, done, _ = self.env.step(action_expert)\r\n\r\n            # Store both\r\n            demo['images'].append(image)\r\n            demo['actions_vla'].append(action_vla)\r\n            demo['actions_expert'].append(action_expert)\r\n\r\n        self.new_demos.append(demo)\r\n        return demo\r\n\r\n    def retrain_vla(self):\r\n        \"\"\"Retrain VLA on mixture of original + corrective demos\"\"\"\r\n        # Mix: 80% original, 20% corrective\r\n        mixed_dataset = self.mix_datasets(\r\n            original_ratio=0.8,\r\n            corrective_ratio=0.2\r\n        )\r\n\r\n        # Fine-tune\r\n        self.vla.train(mixed_dataset, epochs=10)\n"})}),"\n",(0,s.jsx)(n.h2,{id:"exercises",children:"Exercises"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Exercise 4.1"}),": VLA Fine-Tuning"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Collect 100 demonstrations of a custom task in Isaac Sim"}),"\n",(0,s.jsx)(n.li,{children:"Fine-tune OpenVLA on your dataset"}),"\n",(0,s.jsx)(n.li,{children:"Evaluate on held-out test set (20 trials)"}),"\n",(0,s.jsx)(n.li,{children:"Measure success rate before vs after fine-tuning"}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Exercise 4.2"}),": Behavior Tree Design"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:'Design behavior tree for "clean the table" task'}),"\n",(0,s.jsx)(n.li,{children:"Implement in py_trees"}),"\n",(0,s.jsx)(n.li,{children:"Add error recovery (retry logic, fallback behaviors)"}),"\n",(0,s.jsx)(n.li,{children:"Test in simulation (50 trials)"}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Exercise 4.3"}),": Real-World Deployment (Simulation)"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Deploy full autonomous system in Isaac Sim"}),"\n",(0,s.jsx)(n.li,{children:"Run 5 different tasks (fetch, place, navigate, open, stack)"}),"\n",(0,s.jsx)(n.li,{children:"Log all failures and categorize errors"}),"\n",(0,s.jsx)(n.li,{children:"Compute overall success rate"}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Exercise 4.4"}),": DAgger Iteration"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Implement DAgger data collection"}),"\n",(0,s.jsx)(n.li,{children:"Collect 50 corrective demonstrations"}),"\n",(0,s.jsx)(n.li,{children:"Retrain VLA with mixed dataset"}),"\n",(0,s.jsx)(n.li,{children:"Measure improvement in success rate"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"VLA Integration"}),": Fine-tune OpenVLA on humanoid tasks, deploy with ROS 2 policy node\r\n",(0,s.jsx)(n.strong,{children:"Task Planning"}),": Behavior trees for skill composition, error recovery with retries and fallbacks\r\n",(0,s.jsx)(n.strong,{children:"Real-World Testing"}),": Systematic evaluation (success rate, time, errors), iterative improvement with DAgger"]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Module 5 Complete!"})," You now have a complete autonomous humanoid system ready for deployment."]}),"\n",(0,s.jsx)(n.h2,{id:"capstone-project-checklist",children:"Capstone Project Checklist"}),"\n",(0,s.jsxs)(n.p,{children:["\u2705 ",(0,s.jsx)(n.strong,{children:"System Design"})," (Chapter 1):"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Requirements analysis (functional, non-functional)"}),"\n",(0,s.jsx)(n.li,{children:"Hardware and software architecture"}),"\n",(0,s.jsx)(n.li,{children:"12-week development roadmap"}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:["\u2705 ",(0,s.jsx)(n.strong,{children:"Perception Stack"})," (Chapter 2):"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Multi-modal sensor integration (RGB-D, IMU, F/T)"}),"\n",(0,s.jsx)(n.li,{children:"Visual SLAM + object detection pipeline"}),"\n",(0,s.jsx)(n.li,{children:"EKF sensor fusion for state estimation"}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:["\u2705 ",(0,s.jsx)(n.strong,{children:"Control Stack"})," (Chapter 3):"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Whole-body QP controller for multi-task coordination"}),"\n",(0,s.jsx)(n.li,{children:"Balance controller (ZMP or MPC)"}),"\n",(0,s.jsx)(n.li,{children:"Joint-level PID and impedance control"}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:["\u2705 ",(0,s.jsx)(n.strong,{children:"VLA Autonomy"})," (Chapter 4):"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"OpenVLA fine-tuning and deployment"}),"\n",(0,s.jsx)(n.li,{children:"Task planning with behavior trees"}),"\n",(0,s.jsx)(n.li,{children:"Real-world testing and iterative improvement"}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Congratulations!"})," You've completed the Physical AI and Humanoid Robotics book. You're now equipped to build, deploy, and iterate on autonomous humanoid robot systems."]})]})}function p(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(d,{...e})}):d(e)}}}]);
"use strict";(globalThis.webpackChunkphysical_ai_humanoid_robotics_book=globalThis.webpackChunkphysical_ai_humanoid_robotics_book||[]).push([[9909],{28453:(e,n,r)=>{r.d(n,{R:()=>o,x:()=>a});var s=r(96540);const i={},t=s.createContext(i);function o(e){const n=s.useContext(t);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:o(e.components),s.createElement(t.Provider,{value:n},e.children)}},96640:(e,n,r)=>{r.r(n),r.d(n,{assets:()=>l,contentTitle:()=>a,default:()=>m,frontMatter:()=>o,metadata:()=>s,toc:()=>c});const s=JSON.parse('{"id":"chapters/module-5-capstone/perception-stack","title":"Chapter 2: Perception Stack","description":"Sensor integration, SLAM, object detection, and state estimation for autonomous humanoids","source":"@site/docs/chapters/module-5-capstone/02-perception-stack.md","sourceDirName":"chapters/module-5-capstone","slug":"/chapters/module-5-capstone/perception-stack","permalink":"/Physical-AI-Humanoid-Robotics/docs/chapters/module-5-capstone/perception-stack","draft":false,"unlisted":false,"editUrl":"https://github.com/ayeshadev283-max/Physical-AI-Humanoid-Robotics/tree/main/docs/chapters/module-5-capstone/02-perception-stack.md","tags":[{"inline":true,"label":"perception","permalink":"/Physical-AI-Humanoid-Robotics/docs/tags/perception"},{"inline":true,"label":"slam","permalink":"/Physical-AI-Humanoid-Robotics/docs/tags/slam"},{"inline":true,"label":"object-detection","permalink":"/Physical-AI-Humanoid-Robotics/docs/tags/object-detection"},{"inline":true,"label":"state-estimation","permalink":"/Physical-AI-Humanoid-Robotics/docs/tags/state-estimation"},{"inline":true,"label":"sensor-fusion","permalink":"/Physical-AI-Humanoid-Robotics/docs/tags/sensor-fusion"}],"version":"current","sidebarPosition":2,"frontMatter":{"sidebar_position":2,"title":"Chapter 2: Perception Stack","description":"Sensor integration, SLAM, object detection, and state estimation for autonomous humanoids","tags":["perception","slam","object-detection","state-estimation","sensor-fusion"]},"sidebar":"bookSidebar","previous":{"title":"Chapter 1: System Overview","permalink":"/Physical-AI-Humanoid-Robotics/docs/chapters/module-5-capstone/system-overview"},"next":{"title":"Chapter 3: Control Stack","permalink":"/Physical-AI-Humanoid-Robotics/docs/chapters/module-5-capstone/control-stack"}}');var i=r(74848),t=r(28453);const o={sidebar_position:2,title:"Chapter 2: Perception Stack",description:"Sensor integration, SLAM, object detection, and state estimation for autonomous humanoids",tags:["perception","slam","object-detection","state-estimation","sensor-fusion"]},a="Chapter 2: Perception Stack",l={},c=[{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"2.1 Sensor Suite Integration",id:"21-sensor-suite-integration",level:2},{value:"RGB-D Cameras",id:"rgb-d-cameras",level:3},{value:"IMU (Inertial Measurement Unit)",id:"imu-inertial-measurement-unit",level:3},{value:"Force/Torque Sensors",id:"forcetorque-sensors",level:3},{value:"2.2 Perception Pipeline",id:"22-perception-pipeline",level:2},{value:"Visual SLAM",id:"visual-slam",level:3},{value:"Object Detection",id:"object-detection",level:3},{value:"Depth-Based Grasp Pose Estimation",id:"depth-based-grasp-pose-estimation",level:3},{value:"2.3 State Estimation",id:"23-state-estimation",level:2},{value:"Sensor Fusion (EKF)",id:"sensor-fusion-ekf",level:3},{value:"Complete Perception Pipeline",id:"complete-perception-pipeline",level:3},{value:"Perception Quality Metrics",id:"perception-quality-metrics",level:3},{value:"Exercises",id:"exercises",level:2},{value:"Summary",id:"summary",level:2}];function d(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",mermaid:"mermaid",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,t.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.header,{children:(0,i.jsx)(n.h1,{id:"chapter-2-perception-stack",children:"Chapter 2: Perception Stack"})}),"\n",(0,i.jsx)(n.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsx)(n.li,{children:"Integrate multi-modal sensors (RGB-D cameras, IMU, force/torque)"}),"\n",(0,i.jsx)(n.li,{children:"Implement real-time SLAM and object detection pipeline"}),"\n",(0,i.jsx)(n.li,{children:"Fuse sensor data for robust state estimation"}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"21-sensor-suite-integration",children:"2.1 Sensor Suite Integration"}),"\n",(0,i.jsx)(n.h3,{id:"rgb-d-cameras",children:"RGB-D Cameras"}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Intel RealSense D435i"}),": Depth + RGB + IMU"]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Specifications"}),":"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Depth range: 0.3-10m"}),"\n",(0,i.jsx)(n.li,{children:"Resolution: 1280x720 @ 30 FPS"}),"\n",(0,i.jsx)(n.li,{children:"FOV: 87\xb0 x 58\xb0"}),"\n",(0,i.jsx)(n.li,{children:"IMU: BMI055 (accel + gyro)"}),"\n"]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"ROS 2 Integration"}),":"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"# Install RealSense ROS 2 wrapper\r\nsudo apt-get install ros-humble-realsense2-camera\r\n\r\n# Launch camera node\r\nros2 launch realsense2_camera rs_launch.py \\\r\n    enable_depth:=true \\\r\n    enable_color:=true \\\r\n    enable_infra:=false \\\r\n    enable_gyro:=true \\\r\n    enable_accel:=true \\\r\n    depth_module.profile:=640x480x30 \\\r\n    rgb_camera.profile:=640x480x30\n"})}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Published Topics"}),":"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:"/camera/color/image_raw (sensor_msgs/Image)\r\n/camera/depth/image_rect_raw (sensor_msgs/Image)\r\n/camera/aligned_depth_to_color/image_raw (sensor_msgs/Image)\r\n/camera/imu (sensor_msgs/Imu)\r\n/camera/camera_info (sensor_msgs/CameraInfo)\n"})}),"\n",(0,i.jsx)(n.h3,{id:"imu-inertial-measurement-unit",children:"IMU (Inertial Measurement Unit)"}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"BNO055"}),": 9-DOF IMU (accel, gyro, magnetometer)"]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Calibration"}),":"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'import board\r\nimport adafruit_bno055\r\n\r\ni2c = board.I2C()\r\nimu = adafruit_bno055.BNO055_I2C(i2c)\r\n\r\n# Calibration status (0-3 for each)\r\nprint(f"System: {imu.calibration_status[0]}")\r\nprint(f"Gyro: {imu.calibration_status[1]}")\r\nprint(f"Accel: {imu.calibration_status[2]}")\r\nprint(f"Mag: {imu.calibration_status[3]}")\r\n\r\n# Wait until fully calibrated\r\nwhile imu.calibration_status != (3, 3, 3, 3):\r\n    time.sleep(0.1)\r\n\r\n# Read orientation (quaternion)\r\nquat = imu.quaternion\r\nprint(f"Quaternion: {quat}")\n'})}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"ROS 2 Publisher"}),":"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"import rclpy\r\nfrom rclpy.node import Node\r\nfrom sensor_msgs.msg import Imu\r\nimport adafruit_bno055\r\n\r\nclass IMUNode(Node):\r\n    def __init__(self):\r\n        super().__init__('imu_node')\r\n        self.publisher = self.create_publisher(Imu, '/imu/data', 10)\r\n        self.timer = self.create_timer(0.01, self.publish_imu)  # 100 Hz\r\n\r\n        i2c = board.I2C()\r\n        self.imu = adafruit_bno055.BNO055_I2C(i2c)\r\n\r\n    def publish_imu(self):\r\n        msg = Imu()\r\n        msg.header.stamp = self.get_clock().now().to_msg()\r\n        msg.header.frame_id = 'imu_link'\r\n\r\n        # Orientation (quaternion)\r\n        quat = self.imu.quaternion\r\n        if quat is not None:\r\n            msg.orientation.x = quat[1]\r\n            msg.orientation.y = quat[2]\r\n            msg.orientation.z = quat[3]\r\n            msg.orientation.w = quat[0]\r\n\r\n        # Angular velocity\r\n        gyro = self.imu.gyro\r\n        if gyro is not None:\r\n            msg.angular_velocity.x = gyro[0]\r\n            msg.angular_velocity.y = gyro[1]\r\n            msg.angular_velocity.z = gyro[2]\r\n\r\n        # Linear acceleration\r\n        accel = self.imu.linear_acceleration\r\n        if accel is not None:\r\n            msg.linear_acceleration.x = accel[0]\r\n            msg.linear_acceleration.y = accel[1]\r\n            msg.linear_acceleration.z = accel[2]\r\n\r\n        self.publisher.publish(msg)\n"})}),"\n",(0,i.jsx)(n.h3,{id:"forcetorque-sensors",children:"Force/Torque Sensors"}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"ATI Mini45"}),": 6-axis F/T sensor (hands + feet)"]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Use Cases"}),":"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Hands"}),": Grasp force feedback, object weight estimation"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Feet"}),": Ground contact detection, balance control"]}),"\n"]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"ROS 2 Integration"}),":"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"from sensor_msgs.msg import WrenchStamped\r\n\r\nclass FTSensorNode(Node):\r\n    def __init__(self):\r\n        super().__init__('ft_sensor_node')\r\n        self.pub_left = self.create_publisher(WrenchStamped, '/ft/left_hand', 10)\r\n        self.pub_right = self.create_publisher(WrenchStamped, '/ft/right_hand', 10)\r\n\r\n        # Initialize sensor via serial/USB\r\n        self.ft_sensor = ATI_FT_Sensor('/dev/ttyUSB0')\r\n        self.timer = self.create_timer(0.01, self.publish_ft)\r\n\r\n    def publish_ft(self):\r\n        # Read F/T data\r\n        fx, fy, fz, tx, ty, tz = self.ft_sensor.read()\r\n\r\n        msg = WrenchStamped()\r\n        msg.header.stamp = self.get_clock().now().to_msg()\r\n        msg.wrench.force.x = fx\r\n        msg.wrench.force.y = fy\r\n        msg.wrench.force.z = fz\r\n        msg.wrench.torque.x = tx\r\n        msg.wrench.torque.y = ty\r\n        msg.wrench.torque.z = tz\r\n\r\n        self.pub_left.publish(msg)\n"})}),"\n",(0,i.jsx)(n.h2,{id:"22-perception-pipeline",children:"2.2 Perception Pipeline"}),"\n",(0,i.jsx)(n.h3,{id:"visual-slam",children:"Visual SLAM"}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Isaac ROS Visual SLAM"}),": GPU-accelerated ORB-SLAM3"]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Setup"}),":"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"# Install Isaac ROS Visual SLAM\r\nsudo apt-get install ros-humble-isaac-ros-visual-slam\r\n\r\n# Launch\r\nros2 launch isaac_ros_visual_slam isaac_ros_visual_slam.launch.py\n"})}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Configuration"})," (",(0,i.jsx)(n.code,{children:"config/vslam.yaml"}),"):"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-yaml",children:"visual_slam_node:\r\n  ros__parameters:\r\n    enable_imu_fusion: true\r\n    enable_localization_n_mapping: true\r\n    enable_observations_view: true\r\n    enable_landmarks_view: true\r\n    enable_debug_mode: false\r\n\r\n    # Performance\r\n    max_landmarks_per_frame: 500\r\n    num_cameras: 1\r\n\r\n    # IMU settings\r\n    imu_frame: 'imu_link'\r\n    gyroscope_noise_density: 0.001\r\n    accelerometer_noise_density: 0.01\n"})}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Subscribed Topics"}),":"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.code,{children:"/camera/color/image_raw"})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.code,{children:"/camera/camera_info"})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.code,{children:"/imu/data"})}),"\n"]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Published Topics"}),":"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"/visual_slam/tracking/odometry"})," (nav_msgs/Odometry)"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"/visual_slam/tracking/vo_pose"})," (geometry_msgs/PoseStamped)"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"/visual_slam/vis/observations_cloud"})," (sensor_msgs/PointCloud2)"]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"object-detection",children:"Object Detection"}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"YOLOv8 + Segmentation"}),":"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"from ultralytics import YOLO\r\nimport rclpy\r\nfrom rclpy.node import Node\r\nfrom sensor_msgs.msg import Image\r\nfrom vision_msgs.msg import Detection2DArray, Detection2D\r\nfrom cv_bridge import CvBridge\r\n\r\nclass ObjectDetectionNode(Node):\r\n    def __init__(self):\r\n        super().__init__('object_detection_node')\r\n\r\n        # Load YOLOv8 model\r\n        self.model = YOLO('yolov8n.pt')  # Nano for speed\r\n        self.bridge = CvBridge()\r\n\r\n        # Subscribers\r\n        self.image_sub = self.create_subscription(\r\n            Image,\r\n            '/camera/color/image_raw',\r\n            self.image_callback,\r\n            10\r\n        )\r\n\r\n        # Publishers\r\n        self.detection_pub = self.create_publisher(\r\n            Detection2DArray,\r\n            '/detections',\r\n            10\r\n        )\r\n\r\n    def image_callback(self, msg):\r\n        # Convert ROS Image to OpenCV\r\n        cv_image = self.bridge.imgmsg_to_cv2(msg, desired_encoding='rgb8')\r\n\r\n        # Run inference\r\n        results = self.model(cv_image, conf=0.5)\r\n\r\n        # Parse results\r\n        detections_msg = Detection2DArray()\r\n        detections_msg.header = msg.header\r\n\r\n        for result in results:\r\n            for box in result.boxes:\r\n                det = Detection2D()\r\n                det.bbox.center.position.x = (box.xyxy[0][0] + box.xyxy[0][2]) / 2\r\n                det.bbox.center.position.y = (box.xyxy[0][1] + box.xyxy[0][3]) / 2\r\n                det.bbox.size_x = box.xyxy[0][2] - box.xyxy[0][0]\r\n                det.bbox.size_y = box.xyxy[0][3] - box.xyxy[0][1]\r\n\r\n                # Class and confidence\r\n                det.results[0].hypothesis.class_id = str(int(box.cls[0]))\r\n                det.results[0].hypothesis.score = float(box.conf[0])\r\n\r\n                detections_msg.detections.append(det)\r\n\r\n        self.detection_pub.publish(detections_msg)\n"})}),"\n",(0,i.jsx)(n.h3,{id:"depth-based-grasp-pose-estimation",children:"Depth-Based Grasp Pose Estimation"}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"GraspNet-1Billion"})," (or simpler heuristic):"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'import numpy as np\r\nfrom geometry_msgs.msg import PoseStamped\r\n\r\nclass GraspPoseEstimator:\r\n    def __init__(self):\r\n        self.camera_intrinsics = np.array([\r\n            [615.0, 0, 320.0],\r\n            [0, 615.0, 240.0],\r\n            [0, 0, 1.0]\r\n        ])\r\n\r\n    def estimate_grasp(self, depth_image, object_bbox):\r\n        """\r\n        Estimate 6-DOF grasp pose from depth image and object bbox\r\n\r\n        Args:\r\n            depth_image: (H, W) depth in meters\r\n            object_bbox: (x_min, y_min, x_max, y_max)\r\n\r\n        Returns:\r\n            PoseStamped: Grasp pose in camera frame\r\n        """\r\n        x_min, y_min, x_max, y_max = object_bbox\r\n\r\n        # Crop object region\r\n        object_depth = depth_image[y_min:y_max, x_min:x_max]\r\n\r\n        # Find closest point (naive approach)\r\n        min_depth = np.min(object_depth[object_depth > 0])\r\n        y_grasp, x_grasp = np.where(object_depth == min_depth)\r\n        x_grasp = x_grasp[0] + x_min\r\n        y_grasp = y_grasp[0] + y_min\r\n\r\n        # Backproject to 3D\r\n        z = depth_image[y_grasp, x_grasp]\r\n        x = (x_grasp - self.camera_intrinsics[0, 2]) * z / self.camera_intrinsics[0, 0]\r\n        y = (y_grasp - self.camera_intrinsics[1, 2]) * z / self.camera_intrinsics[1, 1]\r\n\r\n        # Grasp pose (top-down grasp)\r\n        pose = PoseStamped()\r\n        pose.header.frame_id = \'camera_color_optical_frame\'\r\n        pose.pose.position.x = x\r\n        pose.pose.position.y = y\r\n        pose.pose.position.z = z\r\n        pose.pose.orientation.x = 0.707\r\n        pose.pose.orientation.y = 0.0\r\n        pose.pose.orientation.z = 0.0\r\n        pose.pose.orientation.w = 0.707  # 90\xb0 pitch for top-down grasp\r\n\r\n        return pose\n'})}),"\n",(0,i.jsx)(n.h2,{id:"23-state-estimation",children:"2.3 State Estimation"}),"\n",(0,i.jsx)(n.h3,{id:"sensor-fusion-ekf",children:"Sensor Fusion (EKF)"}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"robot_localization"})," package for sensor fusion:"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-yaml",children:"# config/ekf.yaml\r\nekf_filter_node:\r\n  ros__parameters:\r\n    frequency: 50.0\r\n    sensor_timeout: 0.1\r\n    two_d_mode: false\r\n\r\n    # Input sources\r\n    odom0: /visual_slam/tracking/odometry\r\n    odom0_config: [true,  true,  true,  # x, y, z\r\n                   false, false, false, # roll, pitch, yaw\r\n                   true,  true,  true,  # vx, vy, vz\r\n                   false, false, false, # vroll, vpitch, vyaw\r\n                   false, false, false] # ax, ay, az\r\n\r\n    imu0: /imu/data\r\n    imu0_config: [false, false, false,  # x, y, z (position from IMU = false)\r\n                  true,  true,  true,   # roll, pitch, yaw\r\n                  false, false, false,  # vx, vy, vz\r\n                  true,  true,  true,   # vroll, vpitch, vyaw\r\n                  true,  true,  true]   # ax, ay, az\r\n\r\n    # Process noise covariance\r\n    process_noise_covariance: [0.05, 0,    0,    ...\r\n                                0,    0.05, 0,    ...\r\n                                ...]\r\n\r\n    # Initial estimate covariance\r\n    initial_estimate_covariance: [1e-9, 0,    0,    ...\r\n                                   0,    1e-9, 0,    ...\r\n                                   ...]\n"})}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Launch"}),":"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"ros2 launch robot_localization ekf.launch.py\n"})}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Output"}),":"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"/odometry/filtered"})," (nav_msgs/Odometry) - Fused pose and velocity"]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"complete-perception-pipeline",children:"Complete Perception Pipeline"}),"\n",(0,i.jsx)(n.mermaid,{value:"graph LR\r\n    Camera[RGB-D Camera] --\x3e SLAM[Visual SLAM]\r\n    Camera --\x3e ObjDet[Object Detection<br/>YOLOv8]\r\n\r\n    SLAM --\x3e Odom[Odometry]\r\n    IMU[IMU] --\x3e EKF[Extended Kalman Filter]\r\n    Odom --\x3e EKF\r\n\r\n    ObjDet --\x3e GraspPose[Grasp Pose<br/>Estimator]\r\n    Camera --\x3e GraspPose\r\n\r\n    EKF --\x3e FilteredState[Filtered State<br/>position, velocity, orientation]\r\n    GraspPose --\x3e ObjectPoses[Object Poses<br/>for manipulation]\r\n\r\n    style SLAM fill:#e1f5ff\r\n    style EKF fill:#ffffcc\r\n    style ObjDet fill:#ffe1e1"}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Figure 2.1"}),": Perception pipeline showing sensor fusion (SLAM + IMU \u2192 EKF) and object understanding (detection + depth \u2192 grasp poses)."]}),"\n",(0,i.jsx)(n.h3,{id:"perception-quality-metrics",children:"Perception Quality Metrics"}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Localization Accuracy"}),":"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"def evaluate_localization(ground_truth_poses, estimated_poses):\r\n    \"\"\"\r\n    Compute localization error metrics\r\n\r\n    Returns:\r\n        dict: {'mean_error': float, 'max_error': float, 'rmse': float}\r\n    \"\"\"\r\n    errors = []\r\n    for gt, est in zip(ground_truth_poses, estimated_poses):\r\n        error = np.linalg.norm(gt.position - est.position)\r\n        errors.append(error)\r\n\r\n    return {\r\n        'mean_error': np.mean(errors),\r\n        'max_error': np.max(errors),\r\n        'rmse': np.sqrt(np.mean(np.array(errors)**2))\r\n    }\n"})}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Object Detection Performance"}),":"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"mAP (mean Average Precision)"}),": >0.7 for 100 household objects"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Inference time"}),": ",(0,i.jsx)(n.code,{children:"<30ms"})," (YOLOv8-nano on Jetson Orin)"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Detection rate"}),": >90% at 1m distance"]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"exercises",children:"Exercises"}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Exercise 2.1"}),": Sensor Integration"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Connect RealSense D435i to ROS 2"}),"\n",(0,i.jsx)(n.li,{children:"Verify all topics are publishing (RGB, depth, IMU)"}),"\n",(0,i.jsx)(n.li,{children:"Visualize in RViz2"}),"\n",(0,i.jsx)(n.li,{children:"Measure actual frame rates"}),"\n"]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Exercise 2.2"}),": Visual SLAM Evaluation"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Run Isaac ROS Visual SLAM in a room"}),"\n",(0,i.jsx)(n.li,{children:"Walk a closed loop (return to start)"}),"\n",(0,i.jsx)(n.li,{children:"Measure loop closure error (drift)"}),"\n",(0,i.jsx)(n.li,{children:"Compare with and without IMU fusion"}),"\n"]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Exercise 2.3"}),": Object Detection Pipeline"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Train/fine-tune YOLOv8 on custom objects (5-10 classes)"}),"\n",(0,i.jsx)(n.li,{children:"Deploy on Jetson Orin"}),"\n",(0,i.jsx)(n.li,{children:"Benchmark inference time and accuracy (mAP)"}),"\n",(0,i.jsx)(n.li,{children:"Integrate with ROS 2 perception stack"}),"\n"]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Exercise 2.4"}),": Grasp Pose Estimation"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Implement depth-based grasp pose estimator"}),"\n",(0,i.jsx)(n.li,{children:"Test on 10 different objects (varied shapes)"}),"\n",(0,i.jsx)(n.li,{children:"Measure success rate in simulation"}),"\n",(0,i.jsx)(n.li,{children:"Analyze failure modes (reflective surfaces, thin objects)"}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Sensor Integration"}),": RGB-D cameras (depth + color), IMU (orientation), F/T sensors (contact)\r\n",(0,i.jsx)(n.strong,{children:"Perception Pipeline"}),": Visual SLAM (localization), YOLOv8 (object detection), depth-based grasp estimation\r\n",(0,i.jsx)(n.strong,{children:"State Estimation"}),": EKF sensor fusion (SLAM + IMU) for robust pose and velocity estimates"]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Next"}),": Chapter 3 covers control stack (whole-body control, balance, joint-level control)."]})]})}function m(e={}){const{wrapper:n}={...(0,t.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(d,{...e})}):d(e)}}}]);
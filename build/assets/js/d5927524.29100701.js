"use strict";(globalThis.webpackChunkphysical_ai_humanoid_robotics_book=globalThis.webpackChunkphysical_ai_humanoid_robotics_book||[]).push([[6689],{28453:(n,e,r)=>{r.d(e,{R:()=>l,x:()=>o});var i=r(96540);const s={},a=i.createContext(s);function l(n){const e=i.useContext(a);return i.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function o(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(s):n.components||s:l(n.components),i.createElement(a.Provider,{value:e},n.children)}},98275:(n,e,r)=>{r.r(e),r.d(e,{assets:()=>t,contentTitle:()=>o,default:()=>h,frontMatter:()=>l,metadata:()=>i,toc:()=>c});const i=JSON.parse('{"id":"chapters/module-3-isaac/control-loops","title":"Chapter 4: Control Loops","description":"Reinforcement learning, imitation learning, and hybrid control with Isaac","source":"@site/docs/chapters/module-3-isaac/04-control-loops.md","sourceDirName":"chapters/module-3-isaac","slug":"/chapters/module-3-isaac/control-loops","permalink":"/Physical-AI-Humanoid-Robotics/chapters/module-3-isaac/control-loops","draft":false,"unlisted":false,"editUrl":"https://github.com/ayeshadev283-max/Physical-AI-Humanoid-Robotics/tree/main/docs/chapters/module-3-isaac/04-control-loops.md","tags":[{"inline":true,"label":"reinforcement-learning","permalink":"/Physical-AI-Humanoid-Robotics/tags/reinforcement-learning"},{"inline":true,"label":"imitation-learning","permalink":"/Physical-AI-Humanoid-Robotics/tags/imitation-learning"},{"inline":true,"label":"isaac-gym","permalink":"/Physical-AI-Humanoid-Robotics/tags/isaac-gym"},{"inline":true,"label":"control","permalink":"/Physical-AI-Humanoid-Robotics/tags/control"}],"version":"current","sidebarPosition":4,"frontMatter":{"sidebar_position":4,"title":"Chapter 4: Control Loops","description":"Reinforcement learning, imitation learning, and hybrid control with Isaac","tags":["reinforcement-learning","imitation-learning","isaac-gym","control"]},"sidebar":"bookSidebar","previous":{"title":"Chapter 3: Sim-to-Real Transfer","permalink":"/Physical-AI-Humanoid-Robotics/chapters/module-3-isaac/sim-to-real"},"next":{"title":"Module 4: VLA Models","permalink":"/Physical-AI-Humanoid-Robotics/module-4-vla"}}');var s=r(74848),a=r(28453);const l={sidebar_position:4,title:"Chapter 4: Control Loops",description:"Reinforcement learning, imitation learning, and hybrid control with Isaac",tags:["reinforcement-learning","imitation-learning","isaac-gym","control"]},o="Chapter 4: Control Loops",t={},c=[{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"4.1 Reinforcement Learning in Isaac Gym",id:"41-reinforcement-learning-in-isaac-gym",level:2},{value:"Isaac Gym Integration",id:"isaac-gym-integration",level:3},{value:"Training with PPO",id:"training-with-ppo",level:3},{value:"Reward Design",id:"reward-design",level:3},{value:"4.2 Imitation Learning Pipelines",id:"42-imitation-learning-pipelines",level:2},{value:"Behavior Cloning",id:"behavior-cloning",level:3},{value:"DAgger (Dataset Aggregation)",id:"dagger-dataset-aggregation",level:3},{value:"Generative Adversarial Imitation Learning (GAIL)",id:"generative-adversarial-imitation-learning-gail",level:3},{value:"4.3 Hybrid Control Architectures",id:"43-hybrid-control-architectures",level:2},{value:"Classical + Learning",id:"classical--learning",level:3},{value:"Residual RL",id:"residual-rl",level:3},{value:"Hierarchical Control",id:"hierarchical-control",level:3},{value:"Exercises",id:"exercises",level:2},{value:"Summary",id:"summary",level:2}];function d(n){const e={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,a.R)(),...n.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(e.header,{children:(0,s.jsx)(e.h1,{id:"chapter-4-control-loops",children:"Chapter 4: Control Loops"})}),"\n",(0,s.jsx)(e.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,s.jsxs)(e.ol,{children:["\n",(0,s.jsx)(e.li,{children:"Train RL policies in Isaac Gym for parallel robot learning"}),"\n",(0,s.jsx)(e.li,{children:"Implement imitation learning pipelines from demonstrations"}),"\n",(0,s.jsx)(e.li,{children:"Design hybrid control architectures combining classical and learning-based methods"}),"\n"]}),"\n",(0,s.jsx)(e.h2,{id:"41-reinforcement-learning-in-isaac-gym",children:"4.1 Reinforcement Learning in Isaac Gym"}),"\n",(0,s.jsx)(e.h3,{id:"isaac-gym-integration",children:"Isaac Gym Integration"}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Isaac Gym"}),": GPU-accelerated RL environment (part of Isaac Sim)"]}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Features"}),":"]}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Parallel Environments"}),": 1000s of robots simultaneously"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Throughput"}),": 100k steps/second (vs 100 steps/second single robot)"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"GPU Tensors"}),": Observations/actions stay on GPU (no CPU transfer)"]}),"\n"]}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Setup"}),":"]}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-python",children:'from omni.isaac.gym.vec_env import VecEnvBase\r\n\r\nclass HumanoidEnv(VecEnvBase):\r\n    def __init__(self, cfg, sim_device, graphics_device_id, headless):\r\n        self.num_envs = cfg["env"]["numEnvs"]  # e.g., 4096\r\n        self.num_obs = 108  # Observation dim\r\n        self.num_actions = 21  # Action dim (joint targets)\r\n\r\n        super().__init__(cfg, sim_device, graphics_device_id, headless)\r\n\r\n    def create_sim(self):\r\n        # Create Isaac Sim physics scene\r\n        self.sim = gymapi.acquire_sim()\r\n        # ... setup\r\n\r\n    def reset(self):\r\n        # Reset 4096 robots in parallel\r\n        self.gym.set_actor_root_state_tensor(self.sim, self.root_states)\r\n        return self.obs\r\n\r\n    def step(self, actions):\r\n        # Apply actions to all robots\r\n        self.gym.set_dof_position_target_tensor(self.sim, actions)\r\n\r\n        # Step physics (all robots in parallel)\r\n        self.gym.simulate(self.sim)\r\n\r\n        # Compute observations and rewards (on GPU)\r\n        self.obs = self.compute_observations()\r\n        self.rew = self.compute_rewards()\r\n        self.done = self.compute_dones()\r\n\r\n        return self.obs, self.rew, self.done, {}\n'})}),"\n",(0,s.jsx)(e.h3,{id:"training-with-ppo",children:"Training with PPO"}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Algorithm"}),": Proximal Policy Optimization (stable, sample-efficient)"]}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-python",children:"from rl_games.algos_torch import torch_ext\r\nfrom rl_games.common import env_configurations, vecenv\r\n\r\n# Register environment\r\nvecenv.register('IsaacHumanoid', lambda cfg: HumanoidEnv(cfg))\r\n\r\n# Training config\r\nconfig = {\r\n    'params': {\r\n        'seed': 42,\r\n        'algo': {\r\n            'name': 'a2c_continuous'\r\n        },\r\n        'model': {\r\n            'name': 'continuous_a2c_logstd'\r\n        },\r\n        'network': {\r\n            'name': 'actor_critic',\r\n            'separate': False,\r\n            'mlp': {\r\n                'units': [256, 256, 128],\r\n                'activation': 'elu'\r\n            }\r\n        },\r\n        'config': {\r\n            'name': 'HumanoidWalk',\r\n            'env_name': 'IsaacHumanoid',\r\n            'num_actors': 4096,\r\n            'num_steps_per_env': 16,\r\n            'minibatch_size': 32768,\r\n            'learning_rate': 3e-4,\r\n            'horizon_length': 16,\r\n            'gamma': 0.99,\r\n            'lam': 0.95\r\n        }\r\n    }\r\n}\r\n\r\n# Train\r\nrunner = Runner()\r\nrunner.load(config)\r\nrunner.run({'train': True, 'play': False, 'checkpoint': 'runs/'})\n"})}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Training Time"}),": 1 hour to achieve walking (vs 24 hours single robot)"]}),"\n",(0,s.jsx)(e.h3,{id:"reward-design",children:"Reward Design"}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Sparse"})," vs ",(0,s.jsx)(e.strong,{children:"Dense"})," rewards:"]}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-python",children:"def compute_rewards(self):\r\n    # Dense reward (guide learning)\r\n    forward_vel = self.root_states[:, 7]  # x velocity\r\n    upright = self.root_states[:, 3]  # z-component of quaternion\r\n    energy = torch.sum(torch.abs(self.joint_torques), dim=-1)\r\n\r\n    reward = (\r\n        2.0 * forward_vel  # Encourage forward motion\r\n        + 1.0 * upright  # Stay upright\r\n        - 0.001 * energy  # Minimize energy\r\n        - 5.0 * self.fallen  # Penalty for falling\r\n    )\r\n\r\n    return reward\n"})}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Termination Conditions"}),":"]}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-python",children:"def compute_dones(self):\r\n    # Fallen: torso below threshold\r\n    fallen = self.root_states[:, 2] < 0.3  # z-position\r\n\r\n    # Out of bounds\r\n    out_of_bounds = torch.abs(self.root_states[:, :2]) > 10.0  # x, y\r\n\r\n    # Timeout\r\n    timeout = self.progress_buf >= self.max_episode_length\r\n\r\n    return fallen | out_of_bounds.any(dim=-1) | timeout\n"})}),"\n",(0,s.jsx)(e.h2,{id:"42-imitation-learning-pipelines",children:"4.2 Imitation Learning Pipelines"}),"\n",(0,s.jsx)(e.h3,{id:"behavior-cloning",children:"Behavior Cloning"}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Idea"}),": Learn policy from expert demonstrations"]}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Workflow"}),":"]}),"\n",(0,s.jsxs)(e.ol,{children:["\n",(0,s.jsx)(e.li,{children:"Collect expert demos (teleop, scripted, or human)"}),"\n",(0,s.jsxs)(e.li,{children:["Train policy: ",(0,s.jsx)(e.code,{children:"\u03c0(a|s) \u2248 expert(s)"})]}),"\n",(0,s.jsx)(e.li,{children:"Deploy"}),"\n"]}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Data Collection"}),":"]}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-python",children:"# Teleoperate robot, log state-action pairs\r\ndemos = []\r\nwhile teleoperating:\r\n    state = robot.get_state()  # Joint positions, velocities\r\n    action = gamepad.get_input()  # Desired joint velocities\r\n    demos.append((state, action))\r\n\r\n# Save dataset\r\nnp.save('demos.npy', demos)\n"})}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Training"}),":"]}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-python",children:"import torch\r\nimport torch.nn as nn\r\n\r\nclass BehaviorClonePolicy(nn.Module):\r\n    def __init__(self, state_dim, action_dim):\r\n        super().__init__()\r\n        self.net = nn.Sequential(\r\n            nn.Linear(state_dim, 256),\r\n            nn.ReLU(),\r\n            nn.Linear(256, 256),\r\n            nn.ReLU(),\r\n            nn.Linear(256, action_dim)\r\n        )\r\n\r\n    def forward(self, state):\r\n        return self.net(state)\r\n\r\n# Load demos\r\ndemos = np.load('demos.npy', allow_pickle=True)\r\nstates = torch.tensor([d[0] for d in demos])\r\nactions = torch.tensor([d[1] for d in demos])\r\n\r\n# Train\r\npolicy = BehaviorClonePolicy(state_dim=108, action_dim=21)\r\noptimizer = torch.optim.Adam(policy.parameters(), lr=1e-3)\r\n\r\nfor epoch in range(1000):\r\n    pred_actions = policy(states)\r\n    loss = nn.MSELoss()(pred_actions, actions)\r\n    optimizer.zero_grad()\r\n    loss.backward()\r\n    optimizer.step()\n"})}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Limitation"}),": Distribution mismatch (policy sees states expert never visited)"]}),"\n",(0,s.jsx)(e.h3,{id:"dagger-dataset-aggregation",children:"DAgger (Dataset Aggregation)"}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Idea"}),": Iteratively query expert, expand dataset"]}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-python",children:"# Round 1: Train on initial demos\r\npolicy.train(expert_demos)\r\n\r\nfor i in range(10):  # 10 rounds\r\n    # Deploy policy, collect rollouts\r\n    rollouts = []\r\n    for episode in range(100):\r\n        state = env.reset()\r\n        for t in range(horizon):\r\n            action = policy(state)  # Policy action\r\n            expert_action = expert(state)  # Query expert\r\n            rollouts.append((state, expert_action))  # Use expert action\r\n            state, _, done, _ = env.step(action)\r\n            if done:\r\n                break\r\n\r\n    # Augment dataset\r\n    expert_demos.extend(rollouts)\r\n\r\n    # Retrain\r\n    policy.train(expert_demos)\n"})}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Advantage"}),": Policy learns to recover from its own mistakes"]}),"\n",(0,s.jsx)(e.h3,{id:"generative-adversarial-imitation-learning-gail",children:"Generative Adversarial Imitation Learning (GAIL)"}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Idea"}),": Learn reward function from demos, then RL"]}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-python",children:"# Discriminator: real (expert) vs fake (policy)\r\ndiscriminator = nn.Sequential(\r\n    nn.Linear(state_dim + action_dim, 256),\r\n    nn.ReLU(),\r\n    nn.Linear(256, 1),\r\n    nn.Sigmoid()\r\n)\r\n\r\n# Train discriminator\r\nfor epoch in range(epochs):\r\n    # Expert data\r\n    expert_s, expert_a = sample_expert_demos()\r\n    expert_score = discriminator(torch.cat([expert_s, expert_a], dim=-1))\r\n\r\n    # Policy data\r\n    policy_s, policy_a = sample_policy_rollouts()\r\n    policy_score = discriminator(torch.cat([policy_s, policy_a], dim=-1))\r\n\r\n    # Binary cross-entropy\r\n    loss = -torch.mean(torch.log(expert_score) + torch.log(1 - policy_score))\r\n    # Update discriminator\r\n\r\n# Use discriminator as reward for RL\r\nreward = -torch.log(discriminator(s, a))  # High reward for expert-like behavior\n"})}),"\n",(0,s.jsx)(e.h2,{id:"43-hybrid-control-architectures",children:"4.3 Hybrid Control Architectures"}),"\n",(0,s.jsx)(e.h3,{id:"classical--learning",children:"Classical + Learning"}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Idea"}),": Classical control for stability, learning for adaptation"]}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Example"})," (Quadruped Locomotion):"]}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-python",children:"class HybridController:\r\n    def __init__(self):\r\n        self.classical = PIDController()  # Joint-level PD control\r\n        self.learned = NeuralPolicy()  # Learned foothold planner\r\n\r\n    def control(self, state):\r\n        # Learned: High-level planning\r\n        target_footholds = self.learned.plan_footholds(state)\r\n\r\n        # Classical: Low-level tracking\r\n        joint_targets = self.classical.inverse_kinematics(target_footholds)\r\n\r\n        return joint_targets\n"})}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Advantages"}),":"]}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"Stability guarantees from classical"}),"\n",(0,s.jsx)(e.li,{children:"Adaptability from learning"}),"\n",(0,s.jsx)(e.li,{children:"Interpretable (can inspect classical component)"}),"\n"]}),"\n",(0,s.jsx)(e.h3,{id:"residual-rl",children:"Residual RL"}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Idea"}),": Learning corrects classical controller"]}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-python",children:"class ResidualController:\r\n    def __init__(self):\r\n        self.classical = PDController(kp=100, kd=10)\r\n        self.residual = NeuralNetwork()\r\n\r\n    def control(self, state, target):\r\n        # Classical baseline\r\n        classical_action = self.classical.control(state, target)\r\n\r\n        # Learned correction\r\n        residual_action = self.residual(state, target)\r\n\r\n        # Combined (bound residual)\r\n        return classical_action + 0.1 * residual_action\n"})}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Training"}),": RL trains ",(0,s.jsx)(e.code,{children:"residual"}),", ",(0,s.jsx)(e.code,{children:"classical"})," frozen"]}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Use Case"}),": Fine-tune PID gains per environment (rough terrain, slopes)"]}),"\n",(0,s.jsx)(e.h3,{id:"hierarchical-control",children:"Hierarchical Control"}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"High-level"})," (slow): Task planning (learned)\r\n",(0,s.jsx)(e.strong,{children:"Mid-level"})," (medium): Motion primitives (classical or learned)\r\n",(0,s.jsx)(e.strong,{children:"Low-level"})," (fast): Joint control (PD)"]}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-python",children:'class HierarchicalController:\r\n    def __init__(self):\r\n        self.high_level = TaskPlanner()  # Outputs: "walk", "grasp", "stand"\r\n        self.mid_level = MotionPrimitives()  # Outputs: joint trajectories\r\n        self.low_level = PDController()  # Tracks trajectories\r\n\r\n    def control(self, state, goal):\r\n        # High-level: Decide what to do (10 Hz)\r\n        task = self.high_level.plan(state, goal)\r\n\r\n        # Mid-level: Generate trajectory (100 Hz)\r\n        trajectory = self.mid_level.execute(task, state)\r\n\r\n        # Low-level: Track trajectory (1000 Hz)\r\n        torques = self.low_level.track(trajectory, state)\r\n\r\n        return torques\n'})}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Example"}),": Humanoid navigation"]}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"High-level: A* path planning"}),"\n",(0,s.jsx)(e.li,{children:"Mid-level: Footstep planner (learned)"}),"\n",(0,s.jsx)(e.li,{children:"Low-level: Joint PD control"}),"\n"]}),"\n",(0,s.jsx)(e.h2,{id:"exercises",children:"Exercises"}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Exercise 4.1"}),": RL Training in Isaac Gym"]}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"Implement a cartpole balancing task in Isaac Gym (4096 parallel environments)"}),"\n",(0,s.jsx)(e.li,{children:"Train a PPO policy for 1000 episodes"}),"\n",(0,s.jsx)(e.li,{children:"Experiment with reward shaping (balance angle, velocity penalties)"}),"\n",(0,s.jsx)(e.li,{children:"Plot learning curves and analyze convergence"}),"\n"]}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Exercise 4.2"}),": Behavior Cloning for Grasping"]}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"Collect 500 expert demonstrations of pick-and-place in Isaac Sim (teleoperation or scripted)"}),"\n",(0,s.jsx)(e.li,{children:"Train behavior cloning policy (vision \u2192 gripper pose)"}),"\n",(0,s.jsx)(e.li,{children:"Evaluate on test objects not seen during training"}),"\n",(0,s.jsx)(e.li,{children:"Measure success rate and failure modes"}),"\n"]}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Exercise 4.3"}),": DAgger Interactive Learning"]}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"Start with initial BC policy from Exercise 4.2"}),"\n",(0,s.jsx)(e.li,{children:"Implement DAgger: deploy policy, collect expert corrections, retrain"}),"\n",(0,s.jsx)(e.li,{children:"Run 5 iterations of DAgger"}),"\n",(0,s.jsx)(e.li,{children:"Plot success rate vs iteration and compare with pure BC"}),"\n"]}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Exercise 4.4"}),": Hybrid Residual Controller"]}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"Implement a PD controller for quadruped standing balance"}),"\n",(0,s.jsx)(e.li,{children:"Add learned residual correction for terrain adaptation"}),"\n",(0,s.jsx)(e.li,{children:"Train residual policy on randomized terrain in Isaac Sim"}),"\n",(0,s.jsx)(e.li,{children:"Compare performance: PD-only vs PD+residual on slopes/stairs"}),"\n"]}),"\n",(0,s.jsx)(e.h2,{id:"summary",children:"Summary"}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"RL in Isaac Gym"}),": Parallel training (1000s of robots), PPO for stability, reward engineering\r\n",(0,s.jsx)(e.strong,{children:"Imitation Learning"}),": Behavior cloning (fast, brittle), DAgger (iterative), GAIL (adversarial)\r\n",(0,s.jsx)(e.strong,{children:"Hybrid Control"}),": Classical + learning, residual RL, hierarchical architectures"]}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Module 3 Complete!"})," Next modules cover VLA models (Module 4) and capstone project (Module 5)."]})]})}function h(n={}){const{wrapper:e}={...(0,a.R)(),...n.components};return e?(0,s.jsx)(e,{...n,children:(0,s.jsx)(d,{...n})}):d(n)}}}]);
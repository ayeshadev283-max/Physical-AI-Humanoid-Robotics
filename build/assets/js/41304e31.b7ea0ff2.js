"use strict";(globalThis.webpackChunkphysical_ai_humanoid_robotics_book=globalThis.webpackChunkphysical_ai_humanoid_robotics_book||[]).push([[4074],{28453:(n,e,i)=>{i.d(e,{R:()=>o,x:()=>l});var r=i(96540);const s={},a=r.createContext(s);function o(n){const e=r.useContext(a);return r.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function l(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(s):n.components||s:o(n.components),r.createElement(a.Provider,{value:e},n.children)}},44534:(n,e,i)=>{i.r(e),i.d(e,{assets:()=>t,contentTitle:()=>l,default:()=>h,frontMatter:()=>o,metadata:()=>r,toc:()=>c});const r=JSON.parse('{"id":"chapters/module-4-vla/fundamentals","title":"Chapter 1: VLA Fundamentals","description":"Vision-Language-Action models, transformer architectures, and training paradigms","source":"@site/docs/chapters/module-4-vla/01-fundamentals.md","sourceDirName":"chapters/module-4-vla","slug":"/chapters/module-4-vla/fundamentals","permalink":"/Physical-AI-Humanoid-Robotics/chapters/module-4-vla/fundamentals","draft":false,"unlisted":false,"editUrl":"https://github.com/ayeshadev283-max/Physical-AI-Humanoid-Robotics/tree/main/docs/chapters/module-4-vla/01-fundamentals.md","tags":[{"inline":true,"label":"vla","permalink":"/Physical-AI-Humanoid-Robotics/tags/vla"},{"inline":true,"label":"transformers","permalink":"/Physical-AI-Humanoid-Robotics/tags/transformers"},{"inline":true,"label":"foundation-models","permalink":"/Physical-AI-Humanoid-Robotics/tags/foundation-models"},{"inline":true,"label":"multimodal","permalink":"/Physical-AI-Humanoid-Robotics/tags/multimodal"},{"inline":true,"label":"robotics","permalink":"/Physical-AI-Humanoid-Robotics/tags/robotics"}],"version":"current","sidebarPosition":1,"frontMatter":{"sidebar_position":1,"title":"Chapter 1: VLA Fundamentals","description":"Vision-Language-Action models, transformer architectures, and training paradigms","tags":["vla","transformers","foundation-models","multimodal","robotics"]},"sidebar":"bookSidebar","previous":{"title":"Module 4: VLA Models","permalink":"/Physical-AI-Humanoid-Robotics/module-4-vla"},"next":{"title":"Chapter 2: RT-2 and Open-Source VLA Models","permalink":"/Physical-AI-Humanoid-Robotics/chapters/module-4-vla/rt2-models"}}');var s=i(74848),a=i(28453);const o={sidebar_position:1,title:"Chapter 1: VLA Fundamentals",description:"Vision-Language-Action models, transformer architectures, and training paradigms",tags:["vla","transformers","foundation-models","multimodal","robotics"]},l="Chapter 1: VLA Fundamentals",t={},c=[{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"1.1 Vision-Language-Action Models",id:"11-vision-language-action-models",level:2},{value:"What are VLA Models?",id:"what-are-vla-models",level:3},{value:"Historical Context",id:"historical-context",level:3},{value:"Key Research Papers",id:"key-research-papers",level:3},{value:"VLA Model Taxonomy",id:"vla-model-taxonomy",level:3},{value:"1.2 Transformer Architectures for Robotics",id:"12-transformer-architectures-for-robotics",level:2},{value:"From Language to Actions",id:"from-language-to-actions",level:3},{value:"Architecture Components",id:"architecture-components",level:3},{value:"Full VLA Forward Pass",id:"full-vla-forward-pass",level:3},{value:"Attention Mechanisms",id:"attention-mechanisms",level:3},{value:"1.3 Training Paradigms",id:"13-training-paradigms",level:2},{value:"Imitation Learning (Behavior Cloning)",id:"imitation-learning-behavior-cloning",level:3},{value:"Reinforcement Learning",id:"reinforcement-learning",level:3},{value:"Hybrid: Pretrain + Fine-tune",id:"hybrid-pretrain--fine-tune",level:3},{value:"Training Data Sources",id:"training-data-sources",level:3},{value:"Exercises",id:"exercises",level:2},{value:"Summary",id:"summary",level:2}];function d(n){const e={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",mermaid:"mermaid",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,a.R)(),...n.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(e.header,{children:(0,s.jsx)(e.h1,{id:"chapter-1-vla-fundamentals",children:"Chapter 1: VLA Fundamentals"})}),"\n",(0,s.jsx)(e.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,s.jsxs)(e.ol,{children:["\n",(0,s.jsx)(e.li,{children:"Understand Vision-Language-Action (VLA) models and their role in robotics"}),"\n",(0,s.jsx)(e.li,{children:"Grasp transformer architectures adapted for multimodal robot learning"}),"\n",(0,s.jsx)(e.li,{children:"Compare training paradigms: imitation learning, RL, and hybrid approaches"}),"\n"]}),"\n",(0,s.jsx)(e.h2,{id:"11-vision-language-action-models",children:"1.1 Vision-Language-Action Models"}),"\n",(0,s.jsx)(e.h3,{id:"what-are-vla-models",children:"What are VLA Models?"}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Vision-Language-Action (VLA)"})," models are foundation models that map visual observations and natural language instructions to robot actions."]}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Core Capability"}),': "Pick up the red mug" + camera image \u2192 gripper motion']}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Key Innovation"}),": Generalization across tasks, objects, and environments without task-specific retraining."]}),"\n",(0,s.jsx)(e.h3,{id:"historical-context",children:"Historical Context"}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"2017-2020"}),": Task-specific policies"]}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"Separate models for grasping, navigation, manipulation"}),"\n",(0,s.jsx)(e.li,{children:"Trained from scratch per task (100k+ demonstrations)"}),"\n"]}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"2021-2022"}),": Language-conditioned policies"]}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"CLIPort (2021): Language + vision for pick-and-place"}),"\n",(0,s.jsx)(e.li,{children:"BC-Z (2022): Multi-task behavior cloning with language goals"}),"\n"]}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"2023-Present"}),": Foundation models for robotics"]}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"RT-1 (2023): 130k demonstrations, 700 tasks"}),"\n",(0,s.jsx)(e.li,{children:"RT-2 (2023): Vision-language model \u2192 robot actions"}),"\n",(0,s.jsx)(e.li,{children:"OpenVLA (2024): Open-source 7B parameter VLA"}),"\n",(0,s.jsx)(e.li,{children:"\u03c0\u2080 (Pi-Zero, 2024): Generalist robot policy"}),"\n"]}),"\n",(0,s.jsx)(e.h3,{id:"key-research-papers",children:"Key Research Papers"}),"\n",(0,s.jsxs)(e.ol,{children:["\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"RT-1"}),' (Brohan et al., 2023): "RT-1: Robotics Transformer for Real-World Control at Scale"']}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"Transformer for robot control (not just vision/language)"}),"\n",(0,s.jsx)(e.li,{children:"130k robot trajectories across 700 tasks"}),"\n",(0,s.jsx)(e.li,{children:"97% success on seen tasks, 62% on novel tasks"}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"RT-2"}),' (Brohan et al., 2023): "RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control"']}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"Pretrained vision-language model (PaLI-X) \u2192 robot actions"}),"\n",(0,s.jsx)(e.li,{children:"Leverages internet-scale vision-language data"}),"\n",(0,s.jsx)(e.li,{children:"Emergent capabilities: reasoning about object properties, physics"}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"OpenVLA"}),' (Kim et al., 2024): "OpenVLA: An Open-Source Vision-Language-Action Model"']}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"7B parameter model trained on Open X-Embodiment dataset"}),"\n",(0,s.jsx)(e.li,{children:"970k robot trajectories, 22 robot platforms"}),"\n",(0,s.jsx)(e.li,{children:"Apache 2.0 license (commercial-friendly)"}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Octo"}),' (Ghosh et al., 2024): "Octo: An Open-Source Generalist Robot Policy"']}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"Diffusion-based action prediction"}),"\n",(0,s.jsx)(e.li,{children:"800k trajectories from 9 robot types"}),"\n",(0,s.jsxs)(e.li,{children:["Fine-tunes with ",(0,s.jsx)(e.code,{children:"<1000"})," demonstrations"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(e.h3,{id:"vla-model-taxonomy",children:"VLA Model Taxonomy"}),"\n",(0,s.jsx)(e.mermaid,{value:"graph TB\r\n    VLA[VLA Models]\r\n\r\n    VLA --\x3e Closed[Closed-Source]\r\n    VLA --\x3e Open[Open-Source]\r\n\r\n    Closed --\x3e RT1[RT-1<br/>Google]\r\n    Closed --\x3e RT2[RT-2<br/>Google]\r\n    Closed --\x3e PiZero[\u03c0\u2080<br/>Physical Intelligence]\r\n\r\n    Open --\x3e OpenVLA[OpenVLA<br/>7B params]\r\n    Open --\x3e Octo[Octo<br/>Diffusion]\r\n    Open --\x3e SmolVLA[SmolVLA<br/>2B params]\r\n\r\n    style VLA fill:#e1f5ff\r\n    style Closed fill:#ffe1e1\r\n    style Open fill:#e1ffe1"}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Figure 1.1"}),": VLA model landscape showing closed-source (industry) and open-source (academic/community) approaches."]}),"\n",(0,s.jsx)(e.h2,{id:"12-transformer-architectures-for-robotics",children:"1.2 Transformer Architectures for Robotics"}),"\n",(0,s.jsx)(e.h3,{id:"from-language-to-actions",children:"From Language to Actions"}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Standard Transformer"})," (BERT, GPT):"]}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"Input: Text tokens"}),"\n",(0,s.jsx)(e.li,{children:"Output: Text tokens (next word prediction)"}),"\n"]}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"VLA Transformer"}),":"]}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"Input: Image tokens + language tokens + proprioception (joint angles, gripper state)"}),"\n",(0,s.jsx)(e.li,{children:"Output: Action tokens (joint velocities, gripper commands)"}),"\n"]}),"\n",(0,s.jsx)(e.h3,{id:"architecture-components",children:"Architecture Components"}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"1. Vision Encoder"}),":"]}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-python",children:'from transformers import AutoImageProcessor, AutoModel\r\n\r\n# Vision Transformer (ViT) for image encoding\r\nprocessor = AutoImageProcessor.from_pretrained("google/vit-base-patch16-224")\r\nvision_encoder = AutoModel.from_pretrained("google/vit-base-patch16-224")\r\n\r\n# Process camera image\r\nimage_tokens = vision_encoder(pixel_values=processor(images, return_tensors="pt").pixel_values)\r\n# Output: (batch, 196, 768) - 196 patches, 768-dim embeddings\n'})}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"2. Language Encoder"}),":"]}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-python",children:'from transformers import AutoTokenizer, AutoModel\r\n\r\n# T5 for language encoding\r\ntokenizer = AutoTokenizer.from_pretrained("google/t5-v1_1-base")\r\nlanguage_encoder = AutoModel.from_pretrained("google/t5-v1_1-base")\r\n\r\n# Encode instruction\r\ninstruction = "Pick up the red cup"\r\nlanguage_tokens = language_encoder(**tokenizer(instruction, return_tensors="pt"))\r\n# Output: (batch, seq_len, 768)\n'})}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"3. Multimodal Fusion"}),":"]}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-python",children:"import torch\r\nimport torch.nn as nn\r\n\r\nclass MultimodalFusion(nn.Module):\r\n    def __init__(self, dim=768):\r\n        super().__init__()\r\n        self.cross_attention = nn.MultiheadAttention(dim, num_heads=12)\r\n        self.norm = nn.LayerNorm(dim)\r\n\r\n    def forward(self, image_tokens, language_tokens):\r\n        # Cross-attention: language queries, image keys/values\r\n        fused, _ = self.cross_attention(\r\n            query=language_tokens,\r\n            key=image_tokens,\r\n            value=image_tokens\r\n        )\r\n        return self.norm(fused + language_tokens)  # Residual connection\n"})}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"4. Action Head"}),":"]}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-python",children:"class ActionHead(nn.Module):\r\n    def __init__(self, input_dim=768, action_dim=7):\r\n        super().__init__()\r\n        self.head = nn.Sequential(\r\n            nn.Linear(input_dim, 256),\r\n            nn.ReLU(),\r\n            nn.Linear(256, action_dim)\r\n        )\r\n\r\n    def forward(self, fused_tokens):\r\n        # Pool tokens (mean or CLS token)\r\n        pooled = fused_tokens.mean(dim=1)  # (batch, 768)\r\n        actions = self.head(pooled)  # (batch, 7) - e.g., 6-DOF arm + gripper\r\n        return actions\n"})}),"\n",(0,s.jsx)(e.h3,{id:"full-vla-forward-pass",children:"Full VLA Forward Pass"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-python",children:'class SimpleVLA(nn.Module):\r\n    def __init__(self):\r\n        super().__init__()\r\n        self.vision_encoder = AutoModel.from_pretrained("google/vit-base-patch16-224")\r\n        self.language_encoder = AutoModel.from_pretrained("google/t5-v1_1-base")\r\n        self.fusion = MultimodalFusion(dim=768)\r\n        self.action_head = ActionHead(input_dim=768, action_dim=7)\r\n\r\n    def forward(self, images, instructions, proprioception=None):\r\n        # Encode vision\r\n        image_tokens = self.vision_encoder(pixel_values=images).last_hidden_state\r\n\r\n        # Encode language\r\n        language_tokens = self.language_encoder(**instructions).last_hidden_state\r\n\r\n        # Fuse modalities\r\n        fused = self.fusion(image_tokens, language_tokens)\r\n\r\n        # Predict actions\r\n        actions = self.action_head(fused)\r\n\r\n        return actions  # (batch, action_dim)\n'})}),"\n",(0,s.jsx)(e.h3,{id:"attention-mechanisms",children:"Attention Mechanisms"}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Self-Attention"})," (within modality):"]}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"Vision: Spatial relationships between image patches"}),"\n",(0,s.jsx)(e.li,{children:"Language: Dependencies between words"}),"\n"]}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Cross-Attention"})," (across modalities):"]}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:'Language queries image: "Where is the red cup?"'}),"\n",(0,s.jsx)(e.li,{children:'Image queries language: "What object am I looking at?"'}),"\n"]}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Causal Attention"})," (for autoregressive action prediction):"]}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:["Action at time ",(0,s.jsx)(e.code,{children:"t"})," depends only on past actions (t-1, t-2, ...)"]}),"\n",(0,s.jsx)(e.li,{children:'Prevents "looking into the future" during training'}),"\n"]}),"\n",(0,s.jsx)(e.h2,{id:"13-training-paradigms",children:"1.3 Training Paradigms"}),"\n",(0,s.jsx)(e.h3,{id:"imitation-learning-behavior-cloning",children:"Imitation Learning (Behavior Cloning)"}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Objective"}),": Learn policy \u03c0(a|s, g) from expert demonstrations"]}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Data"}),": (observation, language goal, action) tuples"]}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:'Example: (image of table, "pick up cup", gripper_close)'}),"\n"]}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Loss Function"})," (supervised learning):"]}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-python",children:"import torch.nn.functional as F\r\n\r\ndef imitation_loss(predicted_actions, expert_actions):\r\n    # Mean squared error for continuous actions\r\n    return F.mse_loss(predicted_actions, expert_actions)\n"})}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Advantages"}),":"]}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"Simple to implement"}),"\n",(0,s.jsx)(e.li,{children:"Stable training (no exploration needed)"}),"\n",(0,s.jsx)(e.li,{children:"Leverages human expertise"}),"\n"]}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Disadvantages"}),":"]}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"Requires large demonstration datasets (10k-1M)"}),"\n",(0,s.jsx)(e.li,{children:"Distributional shift: Policy sees states expert never visited"}),"\n",(0,s.jsx)(e.li,{children:"No error correction (drift accumulates)"}),"\n"]}),"\n",(0,s.jsx)(e.h3,{id:"reinforcement-learning",children:"Reinforcement Learning"}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Objective"}),": Maximize cumulative reward \u2211 \u03b3\u1d57 r(s\u209c, a\u209c)"]}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Environment"}),": Robot simulator or real hardware\r\n",(0,s.jsx)(e.strong,{children:"Reward"}),": Task success (1.0) or failure (0.0), shaped rewards"]}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Example"})," (PPO training loop):"]}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-python",children:'from stable_baselines3 import PPO\r\n\r\n# VLA policy wrapped for RL\r\npolicy = VLAPolicyWrapper(vla_model)\r\n\r\n# PPO trainer\r\nagent = PPO("MultiInputPolicy", env, policy=policy, verbose=1)\r\n\r\n# Train\r\nagent.learn(total_timesteps=1_000_000)\n'})}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Advantages"}),":"]}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"Learns from trial and error (no demonstrations needed)"}),"\n",(0,s.jsx)(e.li,{children:"Optimizes directly for task success"}),"\n",(0,s.jsx)(e.li,{children:"Handles novel states via exploration"}),"\n"]}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Disadvantages"}),":"]}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"Sample inefficient (millions of steps)"}),"\n",(0,s.jsx)(e.li,{children:"Sim-to-real gap (simulator \u2260 reality)"}),"\n",(0,s.jsx)(e.li,{children:"Reward engineering challenging"}),"\n"]}),"\n",(0,s.jsx)(e.h3,{id:"hybrid-pretrain--fine-tune",children:"Hybrid: Pretrain + Fine-tune"}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Best of both worlds"}),":"]}),"\n",(0,s.jsxs)(e.ol,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Pretrain"})," with imitation learning on large dataset (100k-1M demos)"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Fine-tune"})," with RL on specific task (1k-10k steps)"]}),"\n"]}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Workflow"}),":"]}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-python",children:'# Step 1: Pretrain on demonstrations\r\nvla_model.train_behavior_cloning(demo_dataset, epochs=100)\r\n\r\n# Step 2: Fine-tune with RL\r\nvla_policy = VLAPolicyWrapper(vla_model)\r\nppo = PPO("MultiInputPolicy", env, policy=vla_policy)\r\nppo.learn(total_timesteps=10_000)  # Much faster than RL from scratch\n'})}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Example"}),": RT-2 pretrained on web data (vision-language), fine-tuned on robot tasks"]}),"\n",(0,s.jsx)(e.h3,{id:"training-data-sources",children:"Training Data Sources"}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Open X-Embodiment Dataset"})," (2023):"]}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"1M+ robot trajectories"}),"\n",(0,s.jsx)(e.li,{children:"22 robot types (arms, mobile manipulators, quadrupeds)"}),"\n",(0,s.jsx)(e.li,{children:"527 skills across diverse tasks"}),"\n",(0,s.jsx)(e.li,{children:"CC BY 4.0 license"}),"\n"]}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Key Datasets"}),":"]}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Bridge V2"}),": 60k trajectories, kitchen tasks"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"FrankaPlay"}),": 50k trajectories, Franka Emika arm"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"DROID"}),": 76k trajectories, 564 skills"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Language-Table"}),": Simulated tabletop manipulation"]}),"\n"]}),"\n",(0,s.jsx)(e.h2,{id:"exercises",children:"Exercises"}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Exercise 1.1"}),": VLA Model Paper Review"]}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"Read RT-2 paper (Brohan et al., 2023)"}),"\n",(0,s.jsx)(e.li,{children:"Summarize: (a) Architecture, (b) Training data, (c) Emergent capabilities"}),"\n",(0,s.jsx)(e.li,{children:"Compare with RT-1: What changed? Why does it generalize better?"}),"\n"]}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Exercise 1.2"}),": Transformer Forward Pass"]}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"Implement a minimal VLA model using the code above"}),"\n",(0,s.jsx)(e.li,{children:'Input: 224x224 image + "pick up cup" instruction'}),"\n",(0,s.jsx)(e.li,{children:"Output: 7-dim action (6-DOF pose + gripper)"}),"\n",(0,s.jsx)(e.li,{children:"Count total parameters and estimate FLOPs"}),"\n"]}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Exercise 1.3"}),": Imitation Learning Baseline"]}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"Load the Bridge V2 dataset (or subset)"}),"\n",(0,s.jsx)(e.li,{children:"Train behavior cloning model (ViT + T5 + action head)"}),"\n",(0,s.jsx)(e.li,{children:"Evaluate on held-out tasks"}),"\n",(0,s.jsx)(e.li,{children:"Plot training loss and validation success rate"}),"\n"]}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Exercise 1.4"}),": Hybrid Training Experiment"]}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"Train baseline: (a) BC only, (b) RL only, (c) BC \u2192 RL fine-tuning"}),"\n",(0,s.jsx)(e.li,{children:"Compare sample efficiency (timesteps to 80% success)"}),"\n",(0,s.jsx)(e.li,{children:"Analyze which approach works best for your task"}),"\n"]}),"\n",(0,s.jsx)(e.h2,{id:"summary",children:"Summary"}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"VLA Models"}),": Foundation models mapping vision + language \u2192 actions, enabling generalization across tasks\r\n",(0,s.jsx)(e.strong,{children:"Transformers for Robotics"}),": ViT (vision) + T5 (language) + cross-attention (fusion) + action head\r\n",(0,s.jsx)(e.strong,{children:"Training Paradigms"}),": Imitation learning (simple, stable), RL (optimal, sample-intensive), hybrid (best results)"]}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Next"}),": Chapter 2 covers RT-2, OpenVLA, and SmolVLA architectures and deployment."]})]})}function h(n={}){const{wrapper:e}={...(0,a.R)(),...n.components};return e?(0,s.jsx)(e,{...n,children:(0,s.jsx)(d,{...n})}):d(n)}}}]);
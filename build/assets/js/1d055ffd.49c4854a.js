"use strict";(globalThis.webpackChunkphysical_ai_humanoid_robotics_book=globalThis.webpackChunkphysical_ai_humanoid_robotics_book||[]).push([[9254],{28453:(n,e,r)=>{r.d(e,{R:()=>o,x:()=>l});var i=r(96540);const s={},t=i.createContext(s);function o(n){const e=i.useContext(t);return i.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function l(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(s):n.components||s:o(n.components),i.createElement(t.Provider,{value:e},n.children)}},91104:(n,e,r)=>{r.r(e),r.d(e,{assets:()=>a,contentTitle:()=>l,default:()=>h,frontMatter:()=>o,metadata:()=>i,toc:()=>c});const i=JSON.parse('{"id":"chapters/module-4-vla/rt2-models","title":"Chapter 2: RT-2 and Open-Source VLA Models","description":"RT-2 architecture, OpenVLA, and SmolVLA for edge deployment","source":"@site/docs/chapters/module-4-vla/02-rt2-models.md","sourceDirName":"chapters/module-4-vla","slug":"/chapters/module-4-vla/rt2-models","permalink":"/Physical-AI-Humanoid-Robotics/chapters/module-4-vla/rt2-models","draft":false,"unlisted":false,"editUrl":"https://github.com/ayeshadev283-max/Physical-AI-Humanoid-Robotics/tree/main/docs/chapters/module-4-vla/02-rt2-models.md","tags":[{"inline":true,"label":"rt-2","permalink":"/Physical-AI-Humanoid-Robotics/tags/rt-2"},{"inline":true,"label":"openvla","permalink":"/Physical-AI-Humanoid-Robotics/tags/openvla"},{"inline":true,"label":"smolvla","permalink":"/Physical-AI-Humanoid-Robotics/tags/smolvla"},{"inline":true,"label":"vision-language-action","permalink":"/Physical-AI-Humanoid-Robotics/tags/vision-language-action"},{"inline":true,"label":"robotics","permalink":"/Physical-AI-Humanoid-Robotics/tags/robotics"}],"version":"current","sidebarPosition":2,"frontMatter":{"sidebar_position":2,"title":"Chapter 2: RT-2 and Open-Source VLA Models","description":"RT-2 architecture, OpenVLA, and SmolVLA for edge deployment","tags":["rt-2","openvla","smolvla","vision-language-action","robotics"]},"sidebar":"bookSidebar","previous":{"title":"Chapter 1: VLA Fundamentals","permalink":"/Physical-AI-Humanoid-Robotics/chapters/module-4-vla/fundamentals"},"next":{"title":"Chapter 3: Policy Integration","permalink":"/Physical-AI-Humanoid-Robotics/chapters/module-4-vla/policy-integration"}}');var s=r(74848),t=r(28453);const o={sidebar_position:2,title:"Chapter 2: RT-2 and Open-Source VLA Models",description:"RT-2 architecture, OpenVLA, and SmolVLA for edge deployment",tags:["rt-2","openvla","smolvla","vision-language-action","robotics"]},l="Chapter 2: RT-2 and Open-Source VLA Models",a={},c=[{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"2.1 RT-2 Architecture",id:"21-rt-2-architecture",level:2},{value:"Core Innovation",id:"core-innovation",level:3},{value:"PaLI-X Base Model",id:"pali-x-base-model",level:3},{value:"Action Tokenization",id:"action-tokenization",level:3},{value:"RT-2 Training Process",id:"rt-2-training-process",level:3},{value:"RT-2 Performance",id:"rt-2-performance",level:3},{value:"2.2 OpenVLA",id:"22-openvla",level:2},{value:"Open-Source VLA",id:"open-source-vla",level:3},{value:"Architecture",id:"architecture",level:3},{value:"Installation and Setup",id:"installation-and-setup",level:3},{value:"Inference Example",id:"inference-example",level:3},{value:"Fine-Tuning on Custom Data",id:"fine-tuning-on-custom-data",level:3},{value:"Open X-Embodiment Dataset",id:"open-x-embodiment-dataset",level:3},{value:"2.3 SmolVLA",id:"23-smolvla",level:2},{value:"Small-Scale VLA for Edge Deployment",id:"small-scale-vla-for-edge-deployment",level:3},{value:"Architecture Optimizations",id:"architecture-optimizations",level:3},{value:"Installation",id:"installation",level:3},{value:"Deployment on Jetson Orin",id:"deployment-on-jetson-orin",level:3},{value:"Model Comparison",id:"model-comparison",level:3},{value:"When to Use Each Model",id:"when-to-use-each-model",level:3},{value:"Exercises",id:"exercises",level:2},{value:"Summary",id:"summary",level:2}];function d(n){const e={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",mermaid:"mermaid",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,t.R)(),...n.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(e.header,{children:(0,s.jsx)(e.h1,{id:"chapter-2-rt-2-and-open-source-vla-models",children:"Chapter 2: RT-2 and Open-Source VLA Models"})}),"\n",(0,s.jsx)(e.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,s.jsxs)(e.ol,{children:["\n",(0,s.jsx)(e.li,{children:"Understand RT-2 architecture and how it leverages web-scale pretraining"}),"\n",(0,s.jsx)(e.li,{children:"Deploy OpenVLA for open-source robot control"}),"\n",(0,s.jsx)(e.li,{children:"Use SmolVLA for resource-constrained edge devices"}),"\n"]}),"\n",(0,s.jsx)(e.h2,{id:"21-rt-2-architecture",children:"2.1 RT-2 Architecture"}),"\n",(0,s.jsx)(e.h3,{id:"core-innovation",children:"Core Innovation"}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"RT-2"})," (Robotics Transformer 2): Vision-language model adapted for robot control"]}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Key Insight"}),": Pretrained vision-language models (VLMs) contain rich world knowledge that transfers to robotics."]}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Architecture Pipeline"}),":"]}),"\n",(0,s.jsx)(e.mermaid,{value:"graph LR\r\n    Image[Robot Camera<br/>224x224 RGB] --\x3e VLM[PaLI-X VLM<br/>55B params]\r\n    Text[Language Instruction<br/>pick red apple] --\x3e VLM\r\n\r\n    VLM --\x3e Tokens[Action Tokens<br/>discretized]\r\n    Tokens --\x3e Detokenize[Detokenize]\r\n    Detokenize --\x3e Actions[Continuous Actions<br/>7-DOF + gripper]\r\n\r\n    Actions --\x3e Robot[Robot Execution]\r\n\r\n    style VLM fill:#e1f5ff\r\n    style Actions fill:#ffe1e1"}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Figure 2.1"}),": RT-2 pipeline showing how a pretrained vision-language model (PaLI-X) is fine-tuned to output discretized action tokens instead of text."]}),"\n",(0,s.jsx)(e.h3,{id:"pali-x-base-model",children:"PaLI-X Base Model"}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"PaLI-X"})," (Pathways Language and Image model):"]}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Vision Encoder"}),": ViT-22B (22 billion parameters)"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Language Decoder"}),": UL2 (32B parameters)"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Total"}),": 55B parameters"]}),"\n"]}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Pretraining"}),":"]}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"WebLI dataset: 10B image-text pairs from the web"}),"\n",(0,s.jsx)(e.li,{children:"Tasks: Image captioning, VQA, object detection (in text form)"}),"\n"]}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Example Capabilities"}),":"]}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:'Input: Image of apple + "What color is this fruit?"'}),"\n",(0,s.jsx)(e.li,{children:'Output: "Red"'}),"\n"]}),"\n",(0,s.jsx)(e.h3,{id:"action-tokenization",children:"Action Tokenization"}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Problem"}),": PaLI-X outputs text, but robots need continuous actions (joint angles, velocities)"]}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Solution"}),": Discretize action space into 256 bins per dimension"]}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Example"})," (7-DOF arm):"]}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-python",children:'import numpy as np\r\n\r\ndef tokenize_action(continuous_action, num_bins=256):\r\n    """\r\n    Convert continuous action to discrete tokens\r\n\r\n    Args:\r\n        continuous_action: (7,) array in [-1, 1]\r\n        num_bins: Number of discrete bins (vocabulary size)\r\n\r\n    Returns:\r\n        tokens: (7,) array of integers in [0, num_bins-1]\r\n    """\r\n    # Clip to valid range\r\n    clipped = np.clip(continuous_action, -1, 1)\r\n\r\n    # Map [-1, 1] \u2192 [0, num_bins-1]\r\n    tokens = ((clipped + 1) / 2 * (num_bins - 1)).astype(int)\r\n\r\n    return tokens\r\n\r\n# Example\r\naction = np.array([0.5, -0.3, 0.8, 0.0, -0.5, 0.2, 1.0])  # Continuous\r\ntokens = tokenize_action(action)\r\nprint(tokens)  # [191, 89, 230, 128, 64, 153, 255]\n'})}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Detokenization"})," (inference):"]}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-python",children:'def detokenize_action(tokens, num_bins=256):\r\n    """\r\n    Convert discrete tokens back to continuous actions\r\n\r\n    Args:\r\n        tokens: (7,) array of integers in [0, num_bins-1]\r\n\r\n    Returns:\r\n        continuous_action: (7,) array in [-1, 1]\r\n    """\r\n    # Map [0, num_bins-1] \u2192 [-1, 1]\r\n    continuous = (tokens / (num_bins - 1)) * 2 - 1\r\n\r\n    return continuous.astype(np.float32)\r\n\r\n# Example\r\ncontinuous = detokenize_action(tokens)\r\nprint(continuous)  # Approximates original action\n'})}),"\n",(0,s.jsx)(e.h3,{id:"rt-2-training-process",children:"RT-2 Training Process"}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Phase 1"}),": Pretrain PaLI-X on web data (done by Google)"]}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"10B image-text pairs"}),"\n",(0,s.jsx)(e.li,{children:"Generalist vision-language understanding"}),"\n"]}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Phase 2"}),": Co-fine-tune on robotics + web data"]}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-python",children:"# Pseudo-code for RT-2 training\r\nfor batch in dataloader:\r\n    # Mix robot data (80%) and web data (20%)\r\n    if random.random() < 0.8:\r\n        # Robot trajectory\r\n        images, instructions, actions = batch\r\n        # Tokenize actions\r\n        action_tokens = tokenize_action(actions)\r\n        # Forward pass: predict action tokens\r\n        predicted_tokens = model(images, instructions)\r\n        loss = cross_entropy_loss(predicted_tokens, action_tokens)\r\n    else:\r\n        # Web VQA data (maintain VLM capabilities)\r\n        images, questions, answers = web_batch\r\n        predicted_text = model(images, questions)\r\n        loss = cross_entropy_loss(predicted_text, answers)\r\n\r\n    loss.backward()\r\n    optimizer.step()\n"})}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Key Benefit"}),": Model retains visual reasoning from pretraining while learning robot control"]}),"\n",(0,s.jsx)(e.h3,{id:"rt-2-performance",children:"RT-2 Performance"}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Benchmark"})," (Google Robot Lab, 2023):"]}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"RT-1"})," (trained from scratch): 62% success on novel tasks"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"RT-2-PaLI-X"}),": 90% success on novel tasks"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Emergent reasoning"}),': "Pick up the extinct animal" \u2192 picks dinosaur toy']}),"\n"]}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Generalization Examples"}),":"]}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:'Novel objects: Identifies "heaviest object" without explicit training'}),"\n",(0,s.jsx)(e.li,{children:'Spatial reasoning: "Move apple to the left of banana"'}),"\n",(0,s.jsx)(e.li,{children:'Affordances: "Pick up object for hammering" \u2192 selects hammer'}),"\n"]}),"\n",(0,s.jsx)(e.h2,{id:"22-openvla",children:"2.2 OpenVLA"}),"\n",(0,s.jsx)(e.h3,{id:"open-source-vla",children:"Open-Source VLA"}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"OpenVLA"})," (2024): First open-source, production-ready VLA model"]}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Key Stats"}),":"]}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Parameters"}),": 7B (similar to Llama 2-7B)"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Training Data"}),": Open X-Embodiment (970k trajectories, 22 robots)"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"License"}),": Apache 2.0 (commercial use allowed)"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Performance"}),": Matches RT-2 on Open X-Embodiment tasks"]}),"\n"]}),"\n",(0,s.jsx)(e.h3,{id:"architecture",children:"Architecture"}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Base Model"}),": Prismatic VLM (7B)"]}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Vision"}),": DINOv2 (ViT-L/14) - 300M params"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Language"}),": Llama 2-7B - 7B params"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Fusion"}),": Gated cross-attention"]}),"\n"]}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Action Head"}),": MLP (2-layer, 256 hidden)"]}),"\n",(0,s.jsx)(e.h3,{id:"installation-and-setup",children:"Installation and Setup"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-bash",children:"# Install OpenVLA\r\npip install openvla\r\n\r\n# Download pretrained model (7B params, ~14GB)\r\npython -m openvla.download --model openvla-7b\n"})}),"\n",(0,s.jsx)(e.h3,{id:"inference-example",children:"Inference Example"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-python",children:'from openvla import OpenVLA\r\nimport torch\r\nfrom PIL import Image\r\n\r\n# Load model\r\ndevice = "cuda" if torch.cuda.is_available() else "cpu"\r\nmodel = OpenVLA.from_pretrained("openvla-7b").to(device)\r\n\r\n# Prepare inputs\r\nimage = Image.open("robot_camera.jpg")  # 224x224 RGB\r\ninstruction = "pick up the red cup"\r\n\r\n# Predict action\r\nwith torch.no_grad():\r\n    action = model.predict_action(\r\n        image=image,\r\n        instruction=instruction,\r\n        unnormalize=True  # Convert to real robot action space\r\n    )\r\n\r\nprint(action)  # (7,) - e.g., [x, y, z, roll, pitch, yaw, gripper]\n'})}),"\n",(0,s.jsx)(e.h3,{id:"fine-tuning-on-custom-data",children:"Fine-Tuning on Custom Data"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-python",children:'from openvla import OpenVLA\r\nfrom openvla.data import RobotDataset\r\nfrom torch.utils.data import DataLoader\r\n\r\n# Load pretrained model\r\nmodel = OpenVLA.from_pretrained("openvla-7b")\r\n\r\n# Prepare your dataset (RLDS format)\r\ndataset = RobotDataset(data_path="my_robot_data/", task="pick_and_place")\r\ndataloader = DataLoader(dataset, batch_size=16, shuffle=True)\r\n\r\n# Fine-tune\r\noptimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\r\n\r\nfor epoch in range(10):\r\n    for batch in dataloader:\r\n        images, instructions, actions = batch\r\n\r\n        # Forward pass\r\n        predicted_actions = model(images, instructions)\r\n        loss = model.compute_loss(predicted_actions, actions)\r\n\r\n        # Backward pass\r\n        loss.backward()\r\n        optimizer.step()\r\n        optimizer.zero_grad()\r\n\r\n    print(f"Epoch {epoch}, Loss: {loss.item():.4f}")\r\n\r\n# Save fine-tuned model\r\nmodel.save_pretrained("openvla-finetuned-pickplace")\n'})}),"\n",(0,s.jsx)(e.h3,{id:"open-x-embodiment-dataset",children:"Open X-Embodiment Dataset"}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Coverage"}),":"]}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"22 robot types"}),": Franka, UR5, Kinova, mobile manipulators"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"527 skills"}),": Grasping, placing, pushing, opening, closing"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Diverse environments"}),": Labs, kitchens, warehouses"]}),"\n"]}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Loading Example"}),":"]}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-python",children:"from openvla.data import load_oxe_dataset\r\n\r\n# Load Bridge V2 dataset (kitchen tasks)\r\ndataset = load_oxe_dataset(\"bridge_v2\", split=\"train\")\r\n\r\n# Inspect sample\r\nsample = dataset[0]\r\nprint(sample.keys())  # ['image', 'instruction', 'action', 'robot_type']\n"})}),"\n",(0,s.jsx)(e.h2,{id:"23-smolvla",children:"2.3 SmolVLA"}),"\n",(0,s.jsx)(e.h3,{id:"small-scale-vla-for-edge-deployment",children:"Small-Scale VLA for Edge Deployment"}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"SmolVLA"}),": Lightweight VLA optimized for edge devices (Jetson, Raspberry Pi)"]}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Key Stats"}),":"]}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Parameters"}),": 2B (vs 7B for OpenVLA)"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Latency"}),": 15ms inference (Jetson Orin) vs 50ms for OpenVLA"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Accuracy"}),": 85% of OpenVLA performance with 30% params"]}),"\n"]}),"\n",(0,s.jsx)(e.h3,{id:"architecture-optimizations",children:"Architecture Optimizations"}),"\n",(0,s.jsxs)(e.ol,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Smaller Vision Encoder"}),": DINOv2-S (22M params) vs DINOv2-L (300M)"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Distilled Language Model"}),": Llama 2-2B (distilled from 7B)"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Quantization"}),": INT8 weights (4x memory reduction)"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Pruning"}),": 30% of least important weights removed"]}),"\n"]}),"\n",(0,s.jsx)(e.h3,{id:"installation",children:"Installation"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-bash",children:"# Install SmolVLA\r\npip install smolvla\r\n\r\n# Download quantized model (INT8, ~2GB)\r\npython -m smolvla.download --model smolvla-2b-int8\n"})}),"\n",(0,s.jsx)(e.h3,{id:"deployment-on-jetson-orin",children:"Deployment on Jetson Orin"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-python",children:'from smolvla import SmolVLA\r\nimport torch\r\n\r\n# Load quantized model\r\nmodel = SmolVLA.from_pretrained(\r\n    "smolvla-2b-int8",\r\n    device="cuda",\r\n    quantization="int8"  # Use TensorRT INT8 kernels\r\n)\r\n\r\n# Inference (optimized for low latency)\r\n@torch.inference_mode()\r\ndef predict_action_fast(image, instruction):\r\n    return model.predict_action(\r\n        image=image,\r\n        instruction=instruction,\r\n        use_kv_cache=True  # Cache attention keys/values\r\n    )\r\n\r\n# Benchmark\r\nimport time\r\ntimes = []\r\nfor _ in range(100):\r\n    start = time.time()\r\n    action = predict_action_fast(test_image, "pick cup")\r\n    times.append(time.time() - start)\r\n\r\nprint(f"Latency: {np.mean(times)*1000:.1f}ms \xb1 {np.std(times)*1000:.1f}ms")\r\n# Expected: ~15ms on Jetson Orin\n'})}),"\n",(0,s.jsx)(e.h3,{id:"model-comparison",children:"Model Comparison"}),"\n",(0,s.jsxs)(e.table,{children:[(0,s.jsx)(e.thead,{children:(0,s.jsxs)(e.tr,{children:[(0,s.jsx)(e.th,{children:"Model"}),(0,s.jsx)(e.th,{children:"Params"}),(0,s.jsx)(e.th,{children:"Memory"}),(0,s.jsx)(e.th,{children:"Latency (Jetson Orin)"}),(0,s.jsx)(e.th,{children:"Success Rate"})]})}),(0,s.jsxs)(e.tbody,{children:[(0,s.jsxs)(e.tr,{children:[(0,s.jsx)(e.td,{children:(0,s.jsx)(e.strong,{children:"RT-2"})}),(0,s.jsx)(e.td,{children:"55B"}),(0,s.jsx)(e.td,{children:"110GB"}),(0,s.jsx)(e.td,{children:"N/A (cloud only)"}),(0,s.jsx)(e.td,{children:"90%"})]}),(0,s.jsxs)(e.tr,{children:[(0,s.jsx)(e.td,{children:(0,s.jsx)(e.strong,{children:"OpenVLA"})}),(0,s.jsx)(e.td,{children:"7B"}),(0,s.jsx)(e.td,{children:"14GB"}),(0,s.jsx)(e.td,{children:"50ms"}),(0,s.jsx)(e.td,{children:"88%"})]}),(0,s.jsxs)(e.tr,{children:[(0,s.jsx)(e.td,{children:(0,s.jsx)(e.strong,{children:"SmolVLA"})}),(0,s.jsx)(e.td,{children:"2B"}),(0,s.jsx)(e.td,{children:"2GB (INT8)"}),(0,s.jsx)(e.td,{children:"15ms"}),(0,s.jsx)(e.td,{children:"75%"})]}),(0,s.jsxs)(e.tr,{children:[(0,s.jsx)(e.td,{children:(0,s.jsx)(e.strong,{children:"SmolVLA-distilled"})}),(0,s.jsx)(e.td,{children:"2B"}),(0,s.jsx)(e.td,{children:"2GB"}),(0,s.jsx)(e.td,{children:"15ms"}),(0,s.jsx)(e.td,{children:"80%"})]})]})]}),"\n",(0,s.jsx)(e.h3,{id:"when-to-use-each-model",children:"When to Use Each Model"}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"RT-2"})," (cloud deployment):"]}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"Highest accuracy needed"}),"\n",(0,s.jsx)(e.li,{children:"Cloud compute available"}),"\n",(0,s.jsx)(e.li,{children:"Complex reasoning tasks"}),"\n"]}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"OpenVLA"})," (workstation/server):"]}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"Open-source requirement"}),"\n",(0,s.jsx)(e.li,{children:"GPU server available (A100, RTX 4090)"}),"\n",(0,s.jsx)(e.li,{children:"Fine-tuning on custom data"}),"\n"]}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"SmolVLA"})," (edge deployment):"]}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:["Real-time control (",(0,s.jsx)(e.code,{children:"<20ms"})," latency)"]}),"\n",(0,s.jsx)(e.li,{children:"Jetson Orin, Raspberry Pi 5"}),"\n",(0,s.jsx)(e.li,{children:"Battery-powered robots"}),"\n"]}),"\n",(0,s.jsx)(e.h2,{id:"exercises",children:"Exercises"}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Exercise 2.1"}),": RT-2 Action Tokenization"]}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"Implement action tokenization with 128 bins (instead of 256)"}),"\n",(0,s.jsx)(e.li,{children:"Measure quantization error vs number of bins (64, 128, 256, 512)"}),"\n",(0,s.jsx)(e.li,{children:"Plot error vs bins and recommend optimal bin count"}),"\n"]}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Exercise 2.2"}),": OpenVLA Inference"]}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"Install OpenVLA and run inference on sample images"}),"\n",(0,s.jsx)(e.li,{children:'Test generalization: "pick the largest object", "move left"'}),"\n",(0,s.jsx)(e.li,{children:"Compare predicted actions with ground truth on Bridge V2"}),"\n"]}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Exercise 2.3"}),": Fine-Tuning OpenVLA"]}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"Collect 100 demonstrations of a custom task (or use simulation)"}),"\n",(0,s.jsx)(e.li,{children:"Fine-tune OpenVLA for 10 epochs"}),"\n",(0,s.jsx)(e.li,{children:"Evaluate on held-out test set (success rate)"}),"\n"]}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Exercise 2.4"}),": SmolVLA Edge Deployment"]}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"Deploy SmolVLA on Jetson Orin or Raspberry Pi 5"}),"\n",(0,s.jsx)(e.li,{children:"Benchmark latency with different batch sizes (1, 4, 8)"}),"\n",(0,s.jsx)(e.li,{children:"Profile memory usage and identify bottlenecks"}),"\n"]}),"\n",(0,s.jsx)(e.h2,{id:"summary",children:"Summary"}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"RT-2"}),": 55B parameter VLM adapted for robotics, achieves 90% success via web-scale pretraining\r\n",(0,s.jsx)(e.strong,{children:"OpenVLA"}),": Open-source 7B VLA matching RT-2 performance, Apache 2.0 license\r\n",(0,s.jsx)(e.strong,{children:"SmolVLA"}),": 2B lightweight VLA for edge deployment, 15ms latency on Jetson"]}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Next"}),": Chapter 3 covers integrating VLA policies with ROS 2 and safety wrappers."]})]})}function h(n={}){const{wrapper:e}={...(0,t.R)(),...n.components};return e?(0,s.jsx)(e,{...n,children:(0,s.jsx)(d,{...n})}):d(n)}}}]);